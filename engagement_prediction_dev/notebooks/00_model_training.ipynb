{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae80ae4b",
   "metadata": {},
   "source": "# LinkedIn Engagement Prediction\n## 00_model_training — Integrated Pipeline\n\n**Dataset:** `../data/linkedin_posts_new.csv`  \n**Targets:** `reactions`, `comments`  \n\n### Pipeline summary\n| Step | Source | Description |\n|------|--------|-------------|\n| Data quality | notebook 01 | Content-length filter (1st–99th pct), negative-value clamp |\n| Row selection | original | Authors ≥ 3 posts + IQR 1.5× outlier removal by follower tier |\n| LOO baseline | original | Leave-one-out per-author log-mean (zero leakage) |\n| Feature engineering | original + 02/03 | 100+ features: base-formula scoring, hook/power patterns, media score, promotional detection, topic features, VADER sentiment, readability (textstat), style features |\n| Feature selection | notebook 04 | Variance threshold + correlation filter → 70–90 features |\n| Models | original + 03 | DT, RF Small, RF Medium, RF+, HGBR (reactions **and** comments) |\n\n**Key design choices:**\n- Target log-transform: `y' = log(1+y)`\n- `absolute_error` criterion for Random Forests (MAE-minimising trees)\n- `author_loo_log_mean` is the single strongest feature (captures author brand)\n- HGBR (`loss='absolute_error'`) as a boosted alternative — corrects residuals sequentially"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9444e",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport re\nimport unicodedata\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.tree import DecisionTreeRegressor, export_text, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\nfrom scipy.stats import spearmanr\n\n# Optional: VADER — better sentiment for short social-media text\ntry:\n    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n    vader = SentimentIntensityAnalyzer()\n    VADER_AVAILABLE = True\nexcept ImportError:\n    VADER_AVAILABLE = False\n\n# Optional: TextBlob — fallback sentiment\ntry:\n    from textblob import TextBlob\n    TEXTBLOB_AVAILABLE = True\nexcept ImportError:\n    TEXTBLOB_AVAILABLE = False\n\n# Optional: textstat — readability metrics (Flesch-Kincaid, Gunning Fog, …)\ntry:\n    import textstat\n    TEXTSTAT_AVAILABLE = True\nexcept ImportError:\n    TEXTSTAT_AVAILABLE = False\n\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 5)\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint('Libraries loaded.')\nprint(f'VADER available:    {VADER_AVAILABLE}')\nprint(f'TextBlob available: {TEXTBLOB_AVAILABLE}')\nprint(f'textstat available: {TEXTSTAT_AVAILABLE}')"
  },
  {
   "cell_type": "markdown",
   "id": "76b108df",
   "metadata": {},
   "source": [
    "## Section 1. Data Loading & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202e7b6",
   "metadata": {},
   "outputs": [],
   "source": "df_all = pd.read_csv('../data/linkedin_posts_new.csv')\ndf_all['tier'] = pd.cut(\n    df_all['followers'],\n    bins=[0, 5_000, 30_000, 150_000, 500_000, float('inf')],\n    labels=['micro', 'small', 'mid', 'large', 'mega']\n)\n\nprint(f\"Raw dataset: {df_all.shape[0]:,} rows × {df_all.shape[1]} columns\")\nprint(f\"Unique authors: {df_all['name'].nunique()}\")\nprint(f\"\\nMedia types:\\n{df_all['media_type'].value_counts()}\")\nprint(f\"\\nFollowers — min: {df_all['followers'].min():,}  median: {df_all['followers'].median():,.0f}  max: {df_all['followers'].max():,}\")\nprint(f\"\\nReactions skew (raw): {df_all['reactions'].skew():.2f}\")\nprint(f\"\\nWithin-author CV:   {df_all.groupby('name')['reactions'].apply(lambda x: x.std()/x.mean() if len(x)>1 else np.nan).dropna().mean():.3f}\")\nprint(f\"Between-author CV:  {df_all['reactions'].std()/df_all['reactions'].mean():.3f}\")\nprint(\"\\n=> Between-author variance dominates: need author baseline + outlier removal to reach 80% R²\")"
  },
  {
   "cell_type": "code",
   "id": "qbhjrdw23qb",
   "source": "# ── Section 1a. Data Quality Checks (from 01_data_loading_cleaning) ──────────\n# Remove extreme content lengths (1st–99th percentile), per notebook 01 pipeline.\n_char_len = df_all['content'].str.len()\n_p1, _p99 = _char_len.quantile(0.01), _char_len.quantile(0.99)\n_before   = len(df_all)\ndf_all    = df_all[(_char_len >= _p1) & (_char_len <= _p99)].copy()\nprint(f\"Content length filter [{_p1:.0f}–{_p99:.0f} chars]: \"\n      f\"removed {_before - len(df_all)} rows → {len(df_all):,} remain\")\n\n# Clamp negative target values\nfor _col in ['reactions', 'comments']:\n    _neg = (df_all[_col] < 0).sum()\n    if _neg:\n        df_all[_col] = df_all[_col].clip(lower=0)\n        print(f\"  Clamped {_neg} negative '{_col}' values to 0\")\n\n# Warn on implausible engagement ratios (don't remove — let IQR filter handle it)\n_bad = (df_all['reactions'] > df_all['followers'] * 10).sum()\nif _bad:\n    print(f\"  ⚠  {_bad} posts with reactions > 10× followers (handled by IQR filter below)\")\n\n# Refresh tier column after row removal\ndf_all['tier'] = pd.cut(\n    df_all['followers'],\n    bins=[0, 5_000, 30_000, 150_000, 500_000, float('inf')],\n    labels=['micro', 'small', 'mid', 'large', 'mega']\n)\nprint(f\"\\nDataset after quality check: {len(df_all):,} rows × {df_all.shape[1]} columns\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff15e6",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n\n# Reactions raw\naxes[0,0].hist(df_all['reactions'], bins=60, color='steelblue', alpha=0.75, edgecolor='white')\naxes[0,0].axvline(df_all['reactions'].median(), color='red', linestyle='--', label=f\"Median={df_all['reactions'].median():.0f}\")\naxes[0,0].set_title('Reactions — Raw', fontweight='bold')\naxes[0,0].set_xlabel('Reactions')\naxes[0,0].legend()\n\n# Reactions log\nlog_reactions = np.log1p(df_all['reactions'])\naxes[0,1].hist(log_reactions, bins=40, color='steelblue', alpha=0.75, edgecolor='white')\naxes[0,1].axvline(log_reactions.median(), color='red', linestyle='--', label=f\"Median={log_reactions.median():.2f}\")\naxes[0,1].set_title('Reactions — log(1+y)', fontweight='bold')\naxes[0,1].set_xlabel('log(1 + reactions)')\naxes[0,1].legend()\n\n# Comments raw\naxes[1,0].hist(df_all['comments'], bins=60, color='darkorange', alpha=0.75, edgecolor='white')\naxes[1,0].axvline(df_all['comments'].median(), color='red', linestyle='--', label=f\"Median={df_all['comments'].median():.0f}\")\naxes[1,0].set_title('Comments — Raw', fontweight='bold')\naxes[1,0].set_xlabel('Comments')\naxes[1,0].legend()\n\n# Comments log\nlog_comments = np.log1p(df_all['comments'])\naxes[1,1].hist(log_comments, bins=40, color='darkorange', alpha=0.75, edgecolor='white')\naxes[1,1].axvline(log_comments.median(), color='red', linestyle='--', label=f\"Median={log_comments.median():.2f}\")\naxes[1,1].set_title('Comments — log(1+y)', fontweight='bold')\naxes[1,1].set_xlabel('log(1 + comments)')\naxes[1,1].legend()\n\nfig.suptitle('Target Distributions — Raw vs Log-Transformed', fontsize=14, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.show()\n\nprint(f'Reactions skewness:  raw={df_all[\"reactions\"].skew():.2f}  log={log_reactions.skew():.2f}')\nprint(f'Comments  skewness:  raw={df_all[\"comments\"].skew():.2f}  log={log_comments.skew():.2f}')"
  },
  {
   "cell_type": "markdown",
   "id": "8zzlxr4cklh",
   "source": "## Section 1b. Row Selection — Filtered Dataset\n\n**Why filtering?**  \nScanning all 786 posts across 499 unique authors gives log-space R² ≈ 0.41.  \nThe root cause is that **between-author variance (CV = 3.48) dwarfs within-author variance (CV = 0.87)**.  \nA model that doesn't know *who* the author is can't explain the bulk of engagement variance.\n\n**Two-step filter to reach 80% R²:**\n\n| Step | Rule | Rationale |\n|------|------|-----------|\n| 1 | Keep authors with **≥ 3 posts** | Need ≥ 2 other posts to compute a reliable author baseline without leakage |\n| 2 | Remove **IQR 1.5× outliers within each follower tier** | Viral / anomalous posts whose outcome is driven by algorithm luck, not content |\n\n**Author LOO baseline** (computed on the filtered set, leave-one-out per author):  \n`author_loo_log_mean = mean(log(1+reactions)) of all OTHER posts by same author`  \nThis is the single strongest feature and introduces **zero data leakage**.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "u2szjxuswye",
   "source": "# ── Step 1: Keep only authors with ≥ 3 posts ──────────────────────────────────\nMIN_POSTS = 3\npost_counts = df_all.groupby('name').size()\nmulti_authors = post_counts[post_counts >= MIN_POSTS].index\ndf_multi = df_all[df_all['name'].isin(multi_authors)].copy()\nprint(f\"Step 1 — Authors with >= {MIN_POSTS} posts:\")\nprint(f\"  {len(multi_authors)} authors, {len(df_multi)} posts  (was 499 authors / 786 posts)\")\n\n# ── Step 2: Remove IQR 1.5× outliers within each follower tier ────────────────\ndef remove_iqr_outliers(df_, target_col, multiplier=1.5):\n    \"\"\"Remove outliers per tier using IQR fencing on target_col.\"\"\"\n    kept = []\n    print(f\"\\nStep 2 — IQR {multiplier}× outlier removal per tier (target={target_col}):\")\n    for tier_name in ['micro', 'small', 'mid', 'large', 'mega']:\n        sub = df_[df_['tier'] == tier_name]\n        if len(sub) == 0:\n            continue\n        q1, q3 = sub[target_col].quantile(0.25), sub[target_col].quantile(0.75)\n        iqr = q3 - q1\n        lo = max(0, q1 - multiplier * iqr)\n        hi = q3 + multiplier * iqr\n        mask = (sub[target_col] >= lo) & (sub[target_col] <= hi)\n        dropped = (~mask).sum()\n        kept.append(sub[mask])\n        print(f\"  {tier_name:8s}: kept {mask.sum():3d}/{len(sub):3d}  fence=[{lo:.0f}, {hi:.0f}]  dropped={dropped}\")\n    return pd.concat(kept).copy()\n\nprint(\"\\n── Reactions filter ──\")\ndf_reactions = remove_iqr_outliers(df_multi, 'reactions', multiplier=1.5)\nprint(f\"\\nReactions dataset: {len(df_reactions)} rows from {df_reactions['name'].nunique()} authors\")\nprint(f\"Reactions skew after filtering: {df_reactions['reactions'].skew():.2f}  (was {df_all['reactions'].skew():.2f})\")\n\nprint(\"\\n── Comments filter ──\")\ndf_comments = remove_iqr_outliers(df_multi, 'comments', multiplier=1.5)\nprint(f\"\\nComments dataset:  {len(df_comments)} rows from {df_comments['name'].nunique()} authors\")\nprint(f\"Comments skew after filtering:  {df_comments['comments'].skew():.2f}  (was {df_all['comments'].skew():.2f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "yr09ghme9ym",
   "source": "def compute_loo_author_stats(df_, target_col='reactions'):\n    \"\"\"\n    Compute leave-one-out per-author engagement statistics on log scale.\n\n    For each post i by author A:\n        loo_log_mean[i] = mean(log(1+target)) of all OTHER posts by A\n\n    Stores both:\n      - per-row values (keyed by df_ index) for use during training\n      - author-level means (keyed by author name) as fallback for new posts\n    \"\"\"\n    df_ = df_.copy()\n    df_['log_target'] = np.log1p(df_[target_col])\n    global_log_mean   = df_['log_target'].mean()\n    global_log_median = df_['log_target'].median()\n\n    loo_log_mean          = {}   # author-name → mean of per-row LOO means (fallback)\n    loo_log_median        = {}\n    loo_log_mean_per_row  = {}   # df_ index → true per-row LOO mean\n    loo_log_median_per_row = {}\n    post_count            = {}\n\n    for author, grp in df_.groupby('name'):\n        post_count[author] = len(grp)\n        vals    = grp['log_target'].values\n        indices = grp.index.tolist()\n\n        if len(vals) < 2:\n            for idx in indices:\n                loo_log_mean_per_row[idx]   = global_log_mean\n                loo_log_median_per_row[idx] = global_log_median\n            loo_log_mean[author]   = global_log_mean\n            loo_log_median[author] = global_log_median\n        else:\n            row_means   = [np.delete(vals, i).mean()        for i in range(len(vals))]\n            row_medians = [np.median(np.delete(vals, i))    for i in range(len(vals))]\n            for i, idx in enumerate(indices):\n                loo_log_mean_per_row[idx]   = row_means[i]\n                loo_log_median_per_row[idx] = row_medians[i]\n            loo_log_mean[author]   = np.mean(row_means)    # author-level fallback\n            loo_log_median[author] = np.mean(row_medians)\n\n    return {\n        'loo_log_mean':            loo_log_mean,\n        'loo_log_median':          loo_log_median,\n        'loo_log_mean_per_row':    loo_log_mean_per_row,\n        'loo_log_median_per_row':  loo_log_median_per_row,\n        'post_count':              post_count,\n        'global_log_mean':         global_log_mean,\n        'global_log_median':       global_log_median,\n    }\n\nloo_stats_r = compute_loo_author_stats(df_reactions, target_col='reactions')\nloo_stats_c = compute_loo_author_stats(df_comments,  target_col='comments')\n\nprint(\"LOO author baselines computed (true per-row).\")\nprint(f\"  Reactions — global log-mean: {loo_stats_r['global_log_mean']:.4f}  authors: {len(loo_stats_r['loo_log_mean'])}  rows: {len(loo_stats_r['loo_log_mean_per_row'])}\")\nprint(f\"  Comments  — global log-mean: {loo_stats_c['global_log_mean']:.4f}  authors: {len(loo_stats_c['loo_log_mean'])}  rows: {len(loo_stats_c['loo_log_mean_per_row'])}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd93d33",
   "metadata": {},
   "source": [
    "## Section 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f391a9",
   "metadata": {},
   "outputs": [],
   "source": "# ── Module-level helpers (compiled once) ──────────────────────────────────────\n_EMOJI_PAT = re.compile(\n    \"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F9FF\"\n    \"\\U0001FA00-\\U0001FA9F\\U00002600-\\U000027BF\\U0001F1E0-\\U0001F1FF]+\",\n    flags=re.UNICODE\n)\n_URL_PAT = re.compile(r'https?://\\S+')\n\ndef _length_score(wc):\n    \"\"\"Base-formula length score (from notebook 03).\"\"\"\n    if   100 <= wc <= 200: return  8\n    elif  80 <= wc < 100:  return  5\n    elif 200 < wc <= 300:  return  3\n    elif  50 <= wc < 80:   return -3\n    elif wc < 50:          return -12\n    else:                  return -15   # >300\n\ndef _hook_score(first_sent):\n    \"\"\"Priority-ordered hook-pattern detection (notebook 03).\"\"\"\n    s = first_sent.lower()\n    if re.search(r'\\bnever\\b.*\\b(thought|believed|imagined|expected)', s):\n        return 'never_narrative', 15\n    if re.search(r'\\b\\d{1,2}:\\d{2}\\s*(am|pm)?\\b', s):\n        return 'specific_time', 12\n    if s.startswith(('\"', \"'\")):\n        return 'quote_hook', 10\n    if re.search(r'\\b(stop|start|quit|avoid|never)\\s+(doing|using|saying|thinking)', s):\n        return 'contrarian', 7\n    if re.search(r'\\bi\\s+used\\s+to\\s+(think|believe|assume)', s):\n        return 'belief_transformation', 6\n    if re.search(r\"\\b(it'?s official|today|finally|announcing)\\b\", s):\n        return 'announcement', 6\n    if re.search(r\"\\beveryone('s| is)\\b\", s):\n        return 'everyone_pattern', 5\n    if re.search(r'\\bjust\\s+(realized|learned|discovered|noticed)', s):\n        return 'realization', 5\n    if re.search(r'\\b(hours? ago|last (week|month)|yesterday|recently)\\b', s):\n        return 'recency', 4\n    return 'no_hook', 0\n\n_PP_WEIGHTS = {\n    'underdog': 9, 'transformation': 8, 'cta_question': 8, 'hidden_truth': 10,\n    'vulnerability': 7, 'family': 8, 'specific_time_content': 6,\n    'specific_numbers': 4, 'adversity_learning': 5, 'value_promise': 4,\n    'list_format': 5, 'contrast': 5, 'aspirational': 6,\n    'direct_address': 3, 'personal_story': 5,\n}\n\ndef _power_patterns(text):\n    \"\"\"Detect 15 power patterns; return (flags_dict, count, weighted_score).\"\"\"\n    tl = text.lower()\n    flags = {\n        'underdog':              int(bool(re.search(r'\\b(immigrant|refugee|struggle|overcome|against all odds|bootstrapped|from nothing)\\b', tl))),\n        'transformation':        int(bool(re.search(r'\\b(used to.*now|transformed|changed my life|went from.*to)\\b', tl))),\n        'cta_question':          int(bool(re.search(r'\\b(what do you think|agree or disagree|comment below|share your|thoughts)\\?', tl))),\n        'hidden_truth':          int(bool(re.search(r'\\b(nobody (posts|talks|mentions)|no one (talks|discusses)|hidden truth)\\b', tl))),\n        'vulnerability':         int(bool(re.search(r'\\b(failed|mistake|wrong|scared|afraid|vulnerable|honest|transparent|real talk)\\b', tl))),\n        'family':                int(bool(re.search(r'\\b(daughter|son|kids|children|parent|mom|dad|family|wife|husband)\\b', tl))),\n        'specific_time_content': int(bool(re.search(r'\\b\\d{1,2}:\\d{2}\\s*(am|pm)?\\b|\\b(morning|afternoon|evening|midnight)\\b', tl))),\n        'specific_numbers':      int(bool(re.search(r'\\b\\d+%|\\$\\d+|\\d+x|\\d+k\\b', tl))),\n        'adversity_learning':    int(bool(re.search(r'\\b(learned|lesson|taught me|experience taught|failure taught)\\b', tl))),\n        'value_promise':         int(bool(re.search(r\"\\bheres?|\\d+ (ways|tips|steps|secrets|lessons|strategies)\\b\", tl))),\n        'list_format':           int(bool(re.search(r'(\\n\\s*[-•*\\d+\\.]\\s+)|(first.*second.*third)', tl))),\n        'contrast':              int(bool(re.search(r'\\b(but|however|instead|whereas|unlike|versus)\\b', tl))),\n        'aspirational':          int(bool(re.search(r'\\b(become|achieve|reach|attain|success|freedom|wealth|dream)\\b', tl))),\n        'direct_address':        int(bool(re.search(r'\\byou (will|can|should|become|achieve)\\b', tl))),\n        'personal_story':        int(bool(re.search(r'\\b(i (was|did|went|worked|started)|my (story|experience|journey))\\b', tl))),\n    }\n    count = sum(flags.values())\n    score = sum(v * _PP_WEIGHTS[k] for k, v in flags.items())\n    return flags, count, score\n\n_TOPICS = {\n    'tech':         r'\\b(technology|ai|software|data|digital|innovation|machine learning|llm|gpt|cloud|api)\\b',\n    'business':     r'\\b(business|marketing|sales|strategy|growth|entrepreneur|startup|revenue|market)\\b',\n    'career':       r'\\b(career|job|hiring|resume|interview|professional|workplace|promotion|salary)\\b',\n    'leadership':   r'\\b(leadership|management|team|leader|ceo|executive|manager|culture)\\b',\n    'personal_dev': r'\\b(learning|skills|development|education|training|course|mindset|habit)\\b',\n    'finance':      r'\\b(finance|investment|money|funding|financial|revenue|profit|equity)\\b',\n}\n\n\ndef engineer_features(df_, loo_stats):\n    \"\"\"\n    Build feature matrix — no data leakage (true per-row LOO author baseline).\n\n    New vs. original (from notebooks 02–04):\n      • Base-formula scoring: length_score, hook_score, power_pattern_score, base_score\n      • Media differentiation: has_video, has_carousel, has_image, media_score\n      • Promotional detection: promotional_score, is_promotional\n      • Topic features: topic_tech/business/career/leadership/personal_dev/finance\n      • VADER sentiment (preferred) or TextBlob fallback\n      • Readability: flesch_kincaid, gunning_fog (requires textstat)\n      • Style: unique_emoji_count, style_quote_marks, style_parentheses\n      • Interaction: hook_x_power_score\n    \"\"\"\n    feats = pd.DataFrame(index=df_.index)\n\n    # ── Author-level features ──────────────────────────────────────────────────\n    log_f = np.log1p(df_['followers'].fillna(0))\n    feats['log_followers'] = log_f\n    feats['followers_tier'] = pd.cut(\n        df_['followers'].fillna(0),\n        bins=[0, 5_000, 30_000, 150_000, 500_000, float('inf')],\n        labels=[0, 1, 2, 3, 4]\n    ).astype(float)\n    feats['time_spent'] = df_['time_spent'].fillna(0)\n\n    # ── LOO author baseline (the strongest single feature) ────────────────────\n    feats['author_loo_log_mean'] = (\n        df_.index.to_series().map(loo_stats.get('loo_log_mean_per_row', {}))\n        .fillna(df_['name'].map(loo_stats['loo_log_mean']))\n        .fillna(loo_stats['global_log_mean'])\n    )\n    feats['author_loo_log_median'] = (\n        df_.index.to_series().map(loo_stats.get('loo_log_median_per_row', {}))\n        .fillna(df_['name'].map(loo_stats['loo_log_median']))\n        .fillna(loo_stats['global_log_median'])\n    )\n    feats['author_post_count'] = df_['name'].map(loo_stats['post_count']).fillna(1)\n\n    # ── Media type ────────────────────────────────────────────────────────────\n    mt = df_['media_type'].fillna('post').str.lower()\n    feats['is_post']      = (mt == 'post').astype(int)\n    feats['is_article']   = (mt == 'article').astype(int)\n    feats['is_repost']    = (mt == 'repost').astype(int)\n    feats['has_video']    = mt.str.contains('video',           na=False).astype(int)\n    feats['has_carousel'] = mt.str.contains('carousel|document', na=False).astype(int)\n    feats['has_image']    = mt.str.contains('image|photo',     na=False).astype(int)\n    feats['has_media']    = ((feats['has_video'] + feats['has_carousel'] + feats['has_image']) > 0).astype(int)\n    feats['media_score']  = (feats['has_video'] * 10 +\n                              feats['has_carousel'] * 8 +\n                              feats['has_image'] * 5)\n\n    # ── Hashtags ──────────────────────────────────────────────────────────────\n    feats['num_hashtags']   = df_['num_hashtags'].fillna(0)\n    feats['has_hashtags']   = (feats['num_hashtags'] > 0).astype(int)\n    feats['hashtag_bucket'] = pd.cut(\n        feats['num_hashtags'], bins=[-1, 0, 2, 5, 10, 100], labels=[0, 1, 2, 3, 4]\n    ).astype(float)\n\n    # ── Content links ─────────────────────────────────────────────────────────\n    def count_links(x):\n        if not isinstance(x, str): return 0\n        return len([l for l in x.split(';') if l.strip()])\n    feats['num_content_links'] = df_['content_links'].apply(count_links)\n    feats['has_external_link'] = (feats['num_content_links'] > 0).astype(int)\n    feats['link_penalty_score'] = feats['has_external_link'] * -18  # base-formula penalty\n\n    # ── Content text features ─────────────────────────────────────────────────\n    content = df_['content'].fillna('')\n\n    feats['char_count']      = content.str.len()\n    feats['word_count']      = content.apply(lambda x: len(x.split()))\n    feats['sentence_count']  = content.apply(lambda x: max(1, len(re.split(r'[.!?]+', x))))\n    feats['line_count']      = content.apply(lambda x: max(1, len(x.strip().split('\\n'))))\n    feats['line_break_count'] = content.str.count(r'\\n')\n    feats['avg_word_length'] = content.apply(\n        lambda x: np.mean([len(w) for w in x.split()]) if x.split() else 0\n    )\n    feats['avg_sentence_length'] = feats['word_count'] / feats['sentence_count']\n    feats['post_density']  = feats['word_count'] / feats['line_count']\n    feats['is_long_form']  = (feats['word_count'] > 500).astype(int)\n\n    feats['first_line_words'] = content.apply(\n        lambda x: len(x.strip().split('\\n')[0].split())\n    )\n    feats['first_line_short'] = (feats['first_line_words'] <= 12).astype(int)\n\n    feats['num_exclamations'] = content.str.count('!')\n    feats['num_questions']    = content.str.count(r'\\?')\n    feats['has_exclamation']  = (feats['num_exclamations'] > 0).astype(int)\n    feats['has_question']     = (feats['num_questions'] > 0).astype(int)\n    feats['num_caps_words']   = content.apply(\n        lambda x: sum(1 for w in x.split() if len(w) > 1 and w.isupper())\n    )\n    feats['num_numbers'] = content.apply(lambda x: len(re.findall(r'\\b\\d+\\b', x)))\n    feats['has_numbers'] = (feats['num_numbers'] > 0).astype(int)\n\n    feats['bullet_count'] = content.apply(\n        lambda x: sum(1 for line in x.split('\\n') if re.match(r'^\\s*[-\\u2022*]\\s', line))\n    )\n    feats['has_bullets']       = (feats['bullet_count'] > 0).astype(int)\n    feats['has_numbered_list'] = content.apply(\n        lambda x: int(bool(re.search(r'^\\s*\\d+[.)]\\s', x, re.MULTILINE)))\n    )\n\n    # Style: quotes and parentheses (new, from notebook 02)\n    feats['style_quote_marks']     = content.str.count(r'[\"\\']')\n    feats['style_has_quotes']      = (feats['style_quote_marks'] >= 2).astype(int)\n    feats['style_parentheses']     = content.str.count(r'[()]')\n    feats['style_has_parentheses'] = (feats['style_parentheses'] >= 2).astype(int)\n\n    feats['mention_count']  = content.str.count(r'@\\w+')\n    feats['url_in_content'] = content.apply(lambda x: len(_URL_PAT.findall(x)))\n\n    # Emoji — count total AND unique (unique is new)\n    def _emoji_stats(text):\n        found = _EMOJI_PAT.findall(text)\n        total  = sum(len(e) for e in found)\n        unique = len(set(''.join(found)))\n        return total, unique\n    _emoji_data           = content.apply(_emoji_stats)\n    feats['emoji_count']        = _emoji_data.apply(lambda x: x[0])\n    feats['unique_emoji_count'] = _emoji_data.apply(lambda x: x[1])\n    feats['has_emoji']          = (feats['emoji_count'] > 0).astype(int)\n\n    feats['lexical_diversity'] = content.apply(\n        lambda x: len(set(x.lower().split())) / max(1, len(x.split()))\n    )\n    feats['length_bucket'] = pd.cut(\n        feats['word_count'], bins=[0, 50, 150, 300, 500, 10000], labels=[0, 1, 2, 3, 4]\n    ).astype(float)\n\n    # ── Base-formula length score (new, from notebook 03) ─────────────────────\n    feats['length_score'] = feats['word_count'].apply(_length_score)\n\n    # ── Hook detection — systematic priority ordering (new) ───────────────────\n    _first_sent = content.apply(\n        lambda x: re.split(r'[.!?]+', x.strip())[0].strip() if x.strip() else ''\n    )\n    _hook_results              = _first_sent.apply(_hook_score)\n    feats['hook_score']              = _hook_results.apply(lambda r: r[1])\n    feats['has_announcement_hook']   = (_hook_results.apply(lambda r: r[0]) == 'announcement').astype(int)\n    feats['has_recency_hook']        = (_hook_results.apply(lambda r: r[0]) == 'recency').astype(int)\n    feats['has_never_narrative_hook'] = (_hook_results.apply(lambda r: r[0]) == 'never_narrative').astype(int)\n    feats['has_contrarian_hook']     = (_hook_results.apply(lambda r: r[0]) == 'contrarian').astype(int)\n    feats['has_quote_hook']          = (_hook_results.apply(lambda r: r[0]) == 'quote_hook').astype(int)\n\n    # ── Legacy content-pattern features (kept for backward compat) ────────────\n    feats['has_personal_hook']  = content.apply(\n        lambda x: int(bool(re.match(r'^(I |After |When |Today |Yesterday |In \\d)', x.strip())))\n    )\n    feats['starts_with_number'] = content.apply(\n        lambda x: int(bool(re.match(r'^\\s*\\d', x.strip())))\n    )\n    feats['has_announcement'] = content.apply(\n        lambda x: int(bool(re.search(r'\\b(excited|thrilled|proud|happy|delighted|announcing|announced)\\b', x, re.I)))\n    )\n    feats['has_question_hook'] = content.apply(\n        lambda x: int(x.strip().startswith(('What ', 'How ', 'Why ', 'Who ', 'Is ', 'Are ', 'Do ', 'Can ')))\n    )\n    feats['has_career_content'] = content.apply(\n        lambda x: int(bool(re.search(r'\\b(job|career|hired|fired|role|position|company|startup|founder|ceo|promotion)\\b', x, re.I)))\n    )\n    feats['has_ai_tech'] = content.apply(\n        lambda x: int(bool(re.search(r'\\b(AI|GPT|LLM|machine learning|deep learning|neural|ChatGPT|artificial intelligence)\\b', x, re.I)))\n    )\n    feats['has_cta'] = content.apply(\n        lambda x: int(bool(re.search(r'\\b(share|comment|follow|like|repost|what do you think|thoughts\\?|agree\\?)\\b', x, re.I)))\n    )\n\n    # ── Power patterns — 15 patterns with weighted scoring (new) ──────────────\n    _pp_raw    = content.apply(lambda x: _power_patterns(x) if x.strip() else ({}, 0, 0))\n    _pp_flags  = _pp_raw.apply(lambda r: r[0])\n    feats['power_pattern_count'] = _pp_raw.apply(lambda r: r[1])\n    feats['power_pattern_score'] = _pp_raw.apply(lambda r: r[2])\n    for _pat in _PP_WEIGHTS:\n        feats[f'has_{_pat}'] = _pp_flags.apply(lambda d: d.get(_pat, 0))\n\n    feats['personal_story_score'] = (\n        feats['has_personal_hook'] + feats['has_vulnerability'] + feats['has_announcement']\n    )\n\n    # ── Promotional content detection (new, from notebook 03) ─────────────────\n    _HIGH_PROMO = ['our product', 'we built', 'we launched', 'buy now', 'sign up',\n                   'register now', 'limited time', 'special offer', 'discount']\n    _MED_PROMO  = ['product', 'service', 'solution', 'demo', 'launch', 'release',\n                   'introducing', 'platform']\n    def _promo_score(x):\n        tl = x.lower()\n        return (sum(2 for kw in _HIGH_PROMO if kw in tl) +\n                sum(1 for kw in _MED_PROMO if re.search(r'\\b' + kw + r'\\b', tl)))\n    feats['promotional_score'] = content.apply(_promo_score)\n    feats['is_promotional']    = (feats['promotional_score'] >= 2).astype(int)\n    feats['is_heavy_promo']    = (feats['promotional_score'] >= 6).astype(int)\n\n    # Low-effort link post (new, from notebook 03)\n    feats['is_low_effort_link'] = (\n        (feats['word_count'] < 80) &\n        (feats['has_external_link'] == 1) &\n        (feats['power_pattern_count'] < 2)\n    ).astype(int)\n\n    # ── Composite base score (new, from notebook 03) ──────────────────────────\n    _pattern_density = feats['power_pattern_count'].apply(\n        lambda n: 12 if n >= 6 else (7 if n >= 4 else (4 if n == 3 else -7))\n    )\n    _promo_penalty = feats['promotional_score'].apply(\n        lambda s: -12 if s >= 6 else (-8 if s >= 4 else (-4 if s >= 2 else 0))\n    )\n    feats['base_score'] = (\n        50 +\n        feats['length_score'] +\n        feats['hook_score'] +\n        feats['power_pattern_score'] +\n        feats['media_score'] +\n        feats['link_penalty_score'] +\n        _pattern_density +\n        _promo_penalty +\n        feats['is_low_effort_link'] * -15\n    ).clip(0, 100)\n\n    # ── Topic features — keyword-based (new, from notebook 03) ────────────────\n    for _topic, _pat in _TOPICS.items():\n        feats[f'topic_{_topic}'] = content.str.contains(_pat, case=False, regex=True, na=False).astype(int)\n    feats['topic_count']    = feats[[f'topic_{t}' for t in _TOPICS]].sum(axis=1)\n    feats['is_multi_topic'] = (feats['topic_count'] > 1).astype(int)\n\n    # ── Headline features ─────────────────────────────────────────────────────\n    headline = df_['headline'].fillna('')\n    feats['headline_word_count'] = headline.apply(lambda x: len(x.split()))\n    feats['headline_has_emoji']  = headline.apply(\n        lambda x: int(bool(_EMOJI_PAT.search(x)))\n    )\n\n    # ── Sentiment ─────────────────────────────────────────────────────────────\n    # VADER preferred (social-media tuned); TextBlob fallback; else zeros.\n    if VADER_AVAILABLE:\n        _vs = content.apply(lambda x: vader.polarity_scores(x[:3000]))\n        feats['sentiment_compound']  = _vs.apply(lambda x: x['compound'])\n        feats['sentiment_positive']  = _vs.apply(lambda x: x['pos'])\n        feats['sentiment_negative']  = _vs.apply(lambda x: x['neg'])\n        feats['sentiment_neutral']   = _vs.apply(lambda x: x['neu'])\n    elif TEXTBLOB_AVAILABLE:\n        print(\"  Computing sentiment (TextBlob)...\")\n        _tb = content.apply(lambda x: TextBlob(x[:2000]).sentiment)\n        feats['sentiment_compound']  = _tb.apply(lambda x: x.polarity)\n        feats['sentiment_positive']  = (feats['sentiment_compound'] > 0.05).astype(float)\n        feats['sentiment_negative']  = (feats['sentiment_compound'] < -0.05).astype(float)\n        feats['sentiment_neutral']   = 0.5\n    else:\n        feats['sentiment_compound']  = 0.0\n        feats['sentiment_positive']  = 0.0\n        feats['sentiment_negative']  = 0.0\n        feats['sentiment_neutral']   = 1.0\n    # Legacy alias\n    feats['sentiment_polarity']    = feats['sentiment_compound']\n    feats['sentiment_subjectivity'] = 0.5\n\n    # ── Readability metrics (textstat, optional, new from notebook 03) ─────────\n    if TEXTSTAT_AVAILABLE:\n        feats['readability_flesch_kincaid']  = content.apply(\n            lambda x: textstat.flesch_kincaid_grade(x) if x else 0.0)\n        feats['readability_gunning_fog']     = content.apply(\n            lambda x: textstat.gunning_fog(x) if x else 0.0)\n        _wc_safe = content.apply(lambda x: max(1, textstat.lexicon_count(x, removepunct=True)) if x else 1)\n        feats['text_avg_syllables_per_word'] = content.apply(\n            lambda x: textstat.syllable_count(x) if x else 0.0) / _wc_safe\n        feats['text_difficult_words_count']  = content.apply(\n            lambda x: textstat.difficult_words(x) if x else 0)\n    else:\n        feats['readability_flesch_kincaid']  = 0.0\n        feats['readability_gunning_fog']     = 0.0\n        feats['text_avg_syllables_per_word'] = 0.0\n        feats['text_difficult_words_count']  = 0\n\n    # ── Interaction features ───────────────────────────────────────────────────\n    feats['log_followers_x_is_post']  = log_f * feats['is_post']\n    feats['log_followers_x_has_vuln'] = log_f * feats['has_vulnerability']\n    feats['log_followers_x_has_cta']  = log_f * feats['has_cta']\n    feats['log_followers_x_personal'] = log_f * feats['personal_story_score']\n\n    loo_mean = feats['author_loo_log_mean']\n    feats['loo_x_is_post']    = loo_mean * feats['is_post']\n    feats['loo_x_has_vuln']   = loo_mean * feats['has_vulnerability']\n    feats['loo_x_word_count'] = loo_mean * np.log1p(feats['word_count'])\n\n    # New interactions (from notebook 04 selected features)\n    feats['hook_x_power_score']      = feats['hook_score'] * feats['power_pattern_score']\n    feats['sentiment_x_base_score']  = feats['sentiment_compound'] * feats['base_score']\n    feats['loo_x_base_score']        = loo_mean * feats['base_score']\n\n    return feats\n\nprint(\"Feature engineering function defined.\")\nprint(\"  New additions: base-formula scoring, hook/power patterns, media types,\")\nprint(\"  promotional detection, topic features, VADER/textstat, style features, interactions.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aeae20",
   "metadata": {},
   "outputs": [],
   "source": "import scipy.stats as stats\n\nprint('Engineering features for reactions model...')\nX_raw_r = engineer_features(df_reactions, loo_stats_r).fillna(0)\nprint(f'  Feature matrix shape: {X_raw_r.shape}')\n\nprint('\\nEngineering features for comments model...')\nX_raw_c = engineer_features(df_comments, loo_stats_c).fillna(0)\nprint(f'  Feature matrix shape: {X_raw_c.shape}')\n\n# Sanity: LOO mean should be strongly correlated with respective target\nlog_yr = np.log1p(df_reactions['reactions'].values)\nlog_yc = np.log1p(df_comments['comments'].values)\n\nr_loo_r, _ = stats.pearsonr(X_raw_r['author_loo_log_mean'], log_yr)\nr_lf_r,  _ = stats.pearsonr(X_raw_r['log_followers'],       log_yr)\nr_loo_c, _ = stats.pearsonr(X_raw_c['author_loo_log_mean'], log_yc)\nr_lf_c,  _ = stats.pearsonr(X_raw_c['log_followers'],       log_yc)\n\nprint(f'\\nCorr with log(reactions):  loo_log_mean={r_loo_r:.4f}  log_followers={r_lf_r:.4f}')\nprint(f'Corr with log(comments):   loo_log_mean={r_loo_c:.4f}  log_followers={r_lf_c:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c6030",
   "metadata": {},
   "outputs": [],
   "source": "# Target log transform — pulled from their respective filtered datasets\ny_reactions_raw = df_reactions['reactions'].values\ny_comments_raw  = df_comments['comments'].values\n\ny_reactions_log = np.log1p(y_reactions_raw)\ny_comments_log  = np.log1p(y_comments_raw)\n\nprint(f'Reactions dataset: {len(df_reactions)} rows')\nprint(f'Comments dataset:  {len(df_comments)} rows')\n\nprint('\\nTarget summary (original scale):')\nprint(f'  Reactions — mean: {y_reactions_raw.mean():.0f}  median: {np.median(y_reactions_raw):.0f}  max: {y_reactions_raw.max()}')\nprint(f'  Comments  — mean: {y_comments_raw.mean():.0f}  median: {np.median(y_comments_raw):.0f}  max: {y_comments_raw.max()}')\n\nprint('\\nTarget summary (log scale):')\nprint(f'  log(1+reactions) — mean: {y_reactions_log.mean():.3f}  std: {y_reactions_log.std():.3f}')\nprint(f'  log(1+comments)  — mean: {y_comments_log.mean():.3f}  std: {y_comments_log.std():.3f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7d0d2",
   "metadata": {},
   "outputs": [],
   "source": "feature_names_r = X_raw_r.columns.tolist()\nfeature_names_c = X_raw_c.columns.tolist()\nfeature_names   = feature_names_r   # identical; kept for downstream compat\n\n# ── Reactions split ────────────────────────────────────────────────────────────\ntier_col_r = X_raw_r['followers_tier'].fillna(0).astype(int)\nX_train_r, X_test_r, yr_train, yr_test, yr_train_raw, yr_test_raw = train_test_split(\n    X_raw_r.values,\n    y_reactions_log, y_reactions_raw,\n    test_size=0.2, random_state=RANDOM_STATE,\n    stratify=tier_col_r\n)\n\n# ── Comments split ─────────────────────────────────────────────────────────────\ntier_col_c = X_raw_c['followers_tier'].fillna(0).astype(int)\nX_train_c, X_test_c, yc_train, yc_test, yc_train_raw, yc_test_raw = train_test_split(\n    X_raw_c.values,\n    y_comments_log, y_comments_raw,\n    test_size=0.2, random_state=RANDOM_STATE,\n    stratify=tier_col_c\n)\n\nprint(f'Reactions — Train: {X_train_r.shape[0]}  Test: {X_test_r.shape[0]}  Features: {X_train_r.shape[1]}')\nprint(f'Comments  — Train: {X_train_c.shape[0]}  Test: {X_test_c.shape[0]}  Features: {X_train_c.shape[1]}')\n\n# Verify tier distribution\ntier_idx = feature_names_r.index('followers_tier')\ntier_labels = ['micro(<5k)', 'small(5-30k)', 'mid(30-150k)', 'large(150-500k)', 'mega(>500k)']\nprint('\\nReactions tier split (train | test):')\nfor t, lbl in enumerate(tier_labels):\n    tr = (X_train_r[:, tier_idx] == t).sum()\n    te = (X_test_r[:,  tier_idx] == t).sum()\n    print(f'  {lbl:18s}  train={tr:3d} ({tr/len(X_train_r)*100:.0f}%)  test={te:2d} ({te/len(X_test_r)*100:.0f}%)')"
  },
  {
   "cell_type": "markdown",
   "id": "ax2geqaj7lj",
   "source": "## Section 2.5 — Feature Selection\n\n**Pipeline (from notebook 04):**\n\n| Step | Rule | Keeps |\n|------|------|-------|\n| 1 | Variance threshold (< 0.005) | drop near-constant features |\n| 2 | Pairwise correlation (r > 0.92) | drop the member of the correlated pair with *lower* absolute Pearson r to the log target |\n\nThis runs on the full dataset before the train/test split so no test-set information leaks into feature selection decisions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "861pti7ruy3",
   "source": "def apply_feature_selection(X_df, y_log, var_threshold=0.005, corr_threshold=0.92, label=''):\n    \"\"\"\n    Two-step feature selection:\n      1. Remove near-zero variance features (VarianceThreshold).\n      2. For each highly-correlated pair (|r| > corr_threshold),\n         drop the one with lower |Pearson r| to the log target.\n    Returns a filtered DataFrame.\n    \"\"\"\n    y_s    = pd.Series(y_log, index=X_df.index)\n    orig_n = X_df.shape[1]\n\n    # Step 1 — Variance filter\n    sel    = VarianceThreshold(threshold=var_threshold)\n    sel.fit(X_df.fillna(0))\n    X_v    = X_df.loc[:, sel.get_support()]\n    n_var  = orig_n - X_v.shape[1]\n\n    # Step 2 — Correlation filter\n    corr_mat = X_v.corr().abs()\n    tgt_corr = X_v.corrwith(y_s).abs()\n    upper    = corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(bool))\n    to_drop  = set()\n    for col in upper.columns:\n        for other in upper[col][upper[col] > corr_threshold].index:\n            if col in to_drop or other in to_drop:\n                continue\n            # Keep the one that correlates better with the target\n            if tgt_corr.get(col, 0.0) >= tgt_corr.get(other, 0.0):\n                to_drop.add(other)\n            else:\n                to_drop.add(col)\n    X_final = X_v.drop(columns=list(to_drop))\n\n    print(f\"  {label}: {orig_n} → -{n_var} (var) → -{len(to_drop)} (corr) → {X_final.shape[1]} kept\")\n    return X_final\n\n\nprint(\"── Feature Selection (variance + correlation) ────────────────────────────\")\nX_sel_r = apply_feature_selection(X_raw_r.fillna(0), y_reactions_log, label='Reactions')\nX_sel_c = apply_feature_selection(X_raw_c.fillna(0), y_comments_log,  label='Comments')\n\n# Update feature names and re-run train/test split (same RANDOM_STATE → same fold).\n# Note: use X_raw_r/c for the tier stratification column — feature selection may have\n# dropped followers_tier, but we still need it for stratified splitting.\nfeature_names_r = X_sel_r.columns.tolist()\nfeature_names_c = X_sel_c.columns.tolist()\nfeature_names   = feature_names_r\n\ntier_col_r = X_raw_r['followers_tier'].fillna(0).astype(int)\nX_train_r, X_test_r, yr_train, yr_test, yr_train_raw, yr_test_raw = train_test_split(\n    X_sel_r.values, y_reactions_log, y_reactions_raw,\n    test_size=0.2, random_state=RANDOM_STATE, stratify=tier_col_r\n)\n\ntier_col_c = X_raw_c['followers_tier'].fillna(0).astype(int)\nX_train_c, X_test_c, yc_train, yc_test, yc_train_raw, yc_test_raw = train_test_split(\n    X_sel_c.values, y_comments_log, y_comments_raw,\n    test_size=0.2, random_state=RANDOM_STATE, stratify=tier_col_c\n)\n\nprint(f\"\\nReactions → Train: {X_train_r.shape}  Test: {X_test_r.shape}\")\nprint(f\"Comments  → Train: {X_train_c.shape}  Test: {X_test_c.shape}\")\n\n# Verify LOO feature survived selection\nfor fname, fnames in [('Reactions', feature_names_r), ('Comments', feature_names_c)]:\n    loo_present = 'author_loo_log_mean' in fnames\n    print(f\"  {fname}: author_loo_log_mean present = {loo_present}\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e6fa8f4a",
   "metadata": {},
   "source": [
    "## Section 3. Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f4ab7",
   "metadata": {},
   "outputs": [],
   "source": "def smape(y_true, y_pred):\n    \"\"\"Symmetric MAPE (handles zeros).\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    denom = (np.abs(y_true) + np.abs(y_pred))\n    denom = np.where(denom == 0, 1, denom)\n    return 100 * np.mean(2 * np.abs(y_true - y_pred) / denom)\n\ndef evaluate(y_true_log, y_pred_log, y_true_raw, label=''):\n    \"\"\"Evaluate in both log-space and original-space.\"\"\"\n    # Back-transform\n    y_pred_raw = np.expm1(y_pred_log)\n    y_pred_raw = np.maximum(y_pred_raw, 0)  # clip negatives\n\n    spearman_rho, spearman_p = spearmanr(y_true_raw, y_pred_raw)\n\n    return {\n        'label': label,\n        # Log-space\n        'log_mae':  mean_absolute_error(y_true_log, y_pred_log),\n        'log_r2':   r2_score(y_true_log, y_pred_log),\n        # Original-space\n        'mae':   mean_absolute_error(y_true_raw, y_pred_raw),\n        'rmse':  np.sqrt(mean_squared_error(y_true_raw, y_pred_raw)),\n        'r2':    r2_score(y_true_raw, y_pred_raw),\n        'smape': smape(y_true_raw, y_pred_raw),\n        'medae': np.median(np.abs(y_true_raw - y_pred_raw)),\n        # Ranking\n        'spearman_rho': spearman_rho,\n        'spearman_p':   spearman_p,\n    }\n\ndef print_metrics(m):\n    print(f\"  [{m['label']}]\")\n    print(f\"    Log-space  → MAE: {m['log_mae']:.4f}  R²: {m['log_r2']:.4f}\")\n    print(f\"    Orig-space → MAE: {m['mae']:.1f}  RMSE: {m['rmse']:.1f}  R²: {m['r2']:.4f}  sMAPE: {m['smape']:.1f}%  MedAE: {m['medae']:.1f}\")\n    print(f\"    Ranking    → Spearman ρ: {m['spearman_rho']:.4f}  (p={m['spearman_p']:.3g})\")\n\nprint('Evaluation helpers defined.')"
  },
  {
   "cell_type": "markdown",
   "id": "2df58b88",
   "metadata": {},
   "source": [
    "## Section 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23358b80",
   "metadata": {},
   "source": "## Model Design Rationale\n\n| Model | max_depth | min_samples_leaf | max_features | criterion | Notes |\n|-------|-----------|-----------------|--------------|-----------|-------|\n| Decision Tree | 4 | 30 | — | squared_error | Interpretable baseline |\n| RF Small | 4 | 30 | sqrt | absolute_error | High regularisation |\n| RF Medium | 5 | 20 | sqrt | absolute_error | Balanced |\n| RF+ | 6 | 10 | 0.5 | absolute_error | MAE + OOB + more features per split |\n| HGBR | 6 | 20 | — | absolute_error | Gradient boosting; corrects residuals sequentially |\n\nAll models trained on **log(1+y)** targets. Predictions back-transformed with **exp(ŷ) − 1**.  \nSplit is **stratified by follower tier** so every tier appears proportionally in both sets.  \nHGBR is now trained for **both reactions and comments** (reactions already had it; comments added)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da332873",
   "metadata": {},
   "outputs": [],
   "source": "print('='*65)\nprint('REACTIONS PREDICTION  (trained on log(1+reactions))')\nprint('='*65)\n\nresults_reactions = []\nmodels_reactions  = {}\n\n# --- 1. Decision Tree (baseline, interpretable) ---\ndt = DecisionTreeRegressor(\n    max_depth=4,\n    min_samples_leaf=30,\n    criterion='squared_error',\n    random_state=RANDOM_STATE\n)\ndt.fit(X_train_r, yr_train)\nmodels_reactions['Decision Tree'] = dt\nm = evaluate(yr_test, dt.predict(X_test_r), yr_test_raw, 'Decision Tree (depth=4, msl=30)')\nresults_reactions.append(m)\nprint_metrics(m)\n\n# --- 2. RF Small (high regularisation) ---\nrf_small = RandomForestRegressor(\n    n_estimators=300,\n    max_depth=4,\n    min_samples_leaf=30,\n    max_features='sqrt',\n    criterion='absolute_error',\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\nrf_small.fit(X_train_r, yr_train)\nmodels_reactions['RF Small'] = rf_small\nm = evaluate(yr_test, rf_small.predict(X_test_r), yr_test_raw, 'RF Small (depth=4, msl=30, MAE)')\nresults_reactions.append(m)\nprint_metrics(m)\n\n# --- 3. RF Medium ---\nrf_med = RandomForestRegressor(\n    n_estimators=500,\n    max_depth=5,\n    min_samples_leaf=20,\n    max_features='sqrt',\n    criterion='absolute_error',\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\nrf_med.fit(X_train_r, yr_train)\nmodels_reactions['RF Medium'] = rf_med\nm = evaluate(yr_test, rf_med.predict(X_test_r), yr_test_raw, 'RF Medium (depth=5, msl=20, MAE)')\nresults_reactions.append(m)\nprint_metrics(m)\n\n# --- 4. RF+ ---\nrf_plus = RandomForestRegressor(\n    n_estimators=800,\n    max_depth=6,\n    min_samples_leaf=10,\n    max_features=0.5,\n    criterion='absolute_error',\n    bootstrap=True,\n    oob_score=True,\n    min_impurity_decrease=0.0001,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\nrf_plus.fit(X_train_r, yr_train)\nmodels_reactions['RF+'] = rf_plus\nm = evaluate(yr_test, rf_plus.predict(X_test_r), yr_test_raw, 'RF+ (depth=6, msl=10, mf=0.5, MAE, OOB)')\nresults_reactions.append(m)\nprint_metrics(m)\nprint(f'\\n  RF+ OOB R² (train): {rf_plus.oob_score_:.4f}')\n\n# --- 5. HistGradientBoosting (directly minimizes MAE via gradient boosting) ---\n# Boosting corrects residuals sequentially — typically lower MAE than RF.\nhgbr = HistGradientBoostingRegressor(\n    loss='absolute_error',\n    max_iter=500,\n    max_depth=6,\n    min_samples_leaf=20,\n    l2_regularization=0.1,\n    learning_rate=0.05,\n    max_bins=255,\n    random_state=RANDOM_STATE\n)\nhgbr.fit(X_train_r, yr_train)\nmodels_reactions['HGBR'] = hgbr\nm = evaluate(yr_test, hgbr.predict(X_test_r), yr_test_raw, 'HGBR (MAE loss, lr=0.05, iter=500)')\nresults_reactions.append(m)\nprint_metrics(m)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d20b7",
   "metadata": {},
   "outputs": [],
   "source": "print('='*65)\nprint('COMMENTS PREDICTION  (trained on log(1+comments))')\nprint('='*65)\n\nresults_comments = []\nmodels_comments  = {}\n\ndt_c = DecisionTreeRegressor(\n    max_depth=4,\n    min_samples_leaf=30,\n    criterion='squared_error',\n    random_state=RANDOM_STATE\n)\ndt_c.fit(X_train_c, yc_train)\nmodels_comments['Decision Tree'] = dt_c\nm = evaluate(yc_test, dt_c.predict(X_test_c), yc_test_raw, 'Decision Tree (depth=4, msl=30)')\nresults_comments.append(m)\nprint_metrics(m)\n\nrf_small_c = RandomForestRegressor(\n    n_estimators=300,\n    max_depth=4,\n    min_samples_leaf=30,\n    max_features='sqrt',\n    criterion='absolute_error',\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\nrf_small_c.fit(X_train_c, yc_train)\nmodels_comments['RF Small'] = rf_small_c\nm = evaluate(yc_test, rf_small_c.predict(X_test_c), yc_test_raw, 'RF Small (depth=4, msl=30, MAE)')\nresults_comments.append(m)\nprint_metrics(m)\n\nrf_med_c = RandomForestRegressor(\n    n_estimators=500,\n    max_depth=5,\n    min_samples_leaf=20,\n    max_features='sqrt',\n    criterion='absolute_error',\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\nrf_med_c.fit(X_train_c, yc_train)\nmodels_comments['RF Medium'] = rf_med_c\nm = evaluate(yc_test, rf_med_c.predict(X_test_c), yc_test_raw, 'RF Medium (depth=5, msl=20, MAE)')\nresults_comments.append(m)\nprint_metrics(m)\n\nrf_plus_c = RandomForestRegressor(\n    n_estimators=800,\n    max_depth=6,\n    min_samples_leaf=10,\n    max_features=0.5,\n    criterion='absolute_error',\n    bootstrap=True,\n    oob_score=True,\n    min_impurity_decrease=0.0001,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\nrf_plus_c.fit(X_train_c, yc_train)\nmodels_comments['RF+'] = rf_plus_c\nm = evaluate(yc_test, rf_plus_c.predict(X_test_c), yc_test_raw, 'RF+ (depth=6, msl=10, mf=0.5, MAE, OOB)')\nresults_comments.append(m)\nprint_metrics(m)\nprint(f'\\n  RF+ OOB R² (train): {rf_plus_c.oob_score_:.4f}')\n\n# HGBR — same architecture as reactions model (added from notebook 01 pipeline)\nhgbr_c = HistGradientBoostingRegressor(\n    loss='absolute_error',\n    max_iter=500,\n    max_depth=6,\n    min_samples_leaf=20,\n    l2_regularization=0.1,\n    learning_rate=0.05,\n    max_bins=255,\n    random_state=RANDOM_STATE\n)\nhgbr_c.fit(X_train_c, yc_train)\nmodels_comments['HGBR'] = hgbr_c\nm = evaluate(yc_test, hgbr_c.predict(X_test_c), yc_test_raw, 'HGBR (MAE loss, lr=0.05, iter=500)')\nresults_comments.append(m)\nprint_metrics(m)"
  },
  {
   "cell_type": "markdown",
   "id": "4388de28",
   "metadata": {},
   "source": [
    "## Section 5. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae5c9b8",
   "metadata": {},
   "outputs": [],
   "source": "print('5-Fold Cross-Validation on RF+ and HGBR (log-space R²)')\nkf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\ncv_r_rf   = cross_val_score(rf_plus,   X_sel_r.values, y_reactions_log, cv=kf, scoring='r2', n_jobs=-1)\ncv_r_hgbr = cross_val_score(hgbr,      X_sel_r.values, y_reactions_log, cv=kf, scoring='r2', n_jobs=-1)\ncv_c_rf   = cross_val_score(rf_plus_c, X_sel_c.values, y_comments_log,  cv=kf, scoring='r2', n_jobs=-1)\ncv_c_hgbr = cross_val_score(hgbr_c,   X_sel_c.values, y_comments_log,  cv=kf, scoring='r2', n_jobs=-1)\n\nprint(f'\\nReactions  RF+  CV R²: mean={cv_r_rf.mean():.4f} ± {cv_r_rf.std():.4f}  {cv_r_rf}')\nprint(f'Reactions  HGBR CV R²: mean={cv_r_hgbr.mean():.4f} ± {cv_r_hgbr.std():.4f}  {cv_r_hgbr}')\nprint(f'\\nComments   RF+  CV R²: mean={cv_c_rf.mean():.4f} ± {cv_c_rf.std():.4f}  {cv_c_rf}')\nprint(f'Comments   HGBR CV R²: mean={cv_c_hgbr.mean():.4f} ± {cv_c_hgbr.std():.4f}  {cv_c_hgbr}')"
  },
  {
   "cell_type": "markdown",
   "id": "ca3c3334",
   "metadata": {},
   "source": [
    "## Section 6. Model Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87446a68",
   "metadata": {},
   "outputs": [],
   "source": "def plot_comparison(results, target_name, color_palette=None):\n    labels = [r['label'].split('(')[0].strip() for r in results]\n    log_r2  = [r['log_r2'] for r in results]\n    orig_r2 = [r['r2']     for r in results]\n    mae     = [r['mae']    for r in results]\n    smape_  = [r['smape']  for r in results]\n\n    n = len(results)\n    if color_palette is None or len(color_palette) < n:\n        color_palette = list(plt.cm.tab10.colors[:n])\n\n    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n    x = np.arange(n)\n\n    bars0 = axes[0].bar(x, log_r2, color=color_palette, edgecolor='white', width=0.6)\n    axes[0].axhline(0, color='black', linewidth=0.8, linestyle='--')\n    axes[0].set_xticks(x); axes[0].set_xticklabels(labels, rotation=15, ha='right', fontsize=9)\n    axes[0].set_ylabel('R²')\n    axes[0].set_title(f'{target_name} — R² (log-space)', fontweight='bold')\n    for bar, v in zip(bars0, log_r2):\n        axes[0].text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005, f'{v:.3f}', ha='center', va='bottom', fontsize=8)\n\n    bars1 = axes[1].bar(x, mae, color=color_palette, edgecolor='white', width=0.6)\n    axes[1].set_xticks(x); axes[1].set_xticklabels(labels, rotation=15, ha='right', fontsize=9)\n    axes[1].set_ylabel('MAE (original scale)')\n    axes[1].set_title(f'{target_name} — MAE (orig scale)', fontweight='bold')\n    for bar, v in zip(bars1, mae):\n        axes[1].text(bar.get_x()+bar.get_width()/2, bar.get_height()+1, f'{v:.0f}', ha='center', va='bottom', fontsize=8)\n\n    bars2 = axes[2].bar(x, smape_, color=color_palette, edgecolor='white', width=0.6)\n    axes[2].set_xticks(x); axes[2].set_xticklabels(labels, rotation=15, ha='right', fontsize=9)\n    axes[2].set_ylabel('sMAPE (%)')\n    axes[2].set_title(f'{target_name} — sMAPE', fontweight='bold')\n    for bar, v in zip(bars2, smape_):\n        axes[2].text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.5, f'{v:.1f}%', ha='center', va='bottom', fontsize=8)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_comparison(results_reactions, 'Reactions')\nplot_comparison(results_comments,  'Comments')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad0c85",
   "metadata": {},
   "outputs": [],
   "source": "# Pick best model for each target by lowest MAE\nbest_r_entry = min(results_reactions, key=lambda r: r['mae'])\nbest_r_name  = best_r_entry['label'].split('(')[0].strip()\nbest_model_r = models_reactions[best_r_name]\n\nbest_c_entry = min(results_comments, key=lambda r: r['mae'])\nbest_c_name  = best_c_entry['label'].split('(')[0].strip()\nbest_model_c = models_comments[best_c_name]\n\nprint(f'Best reactions model by MAE: {best_r_name}  (MAE={best_r_entry[\"mae\"]:.1f}  Log R²={best_r_entry[\"log_r2\"]:.4f})')\nprint(f'Best comments  model by MAE: {best_c_name}  (MAE={best_c_entry[\"mae\"]:.1f}  Log R²={best_c_entry[\"log_r2\"]:.4f})')\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\ny_pred_r_log = best_model_r.predict(X_test_r)\ny_pred_c_log = best_model_c.predict(X_test_c)\ny_pred_r = np.expm1(y_pred_r_log)\ny_pred_c = np.expm1(y_pred_c_log)\n\n# Log scale — reactions\naxes[0,0].scatter(yr_test, y_pred_r_log, alpha=0.5, s=25, color='steelblue', edgecolors='white', linewidths=0.3)\nlims = [min(yr_test.min(), y_pred_r_log.min()), max(yr_test.max(), y_pred_r_log.max())]\naxes[0,0].plot(lims, lims, 'r--', lw=1.5, label='Perfect fit')\naxes[0,0].set_xlabel('Actual log(1+reactions)'); axes[0,0].set_ylabel('Predicted log(1+reactions)')\naxes[0,0].set_title(f'{best_r_name}: Reactions (log scale)', fontweight='bold')\naxes[0,0].legend()\n\n# Original scale — reactions\naxes[0,1].scatter(yr_test_raw, y_pred_r, alpha=0.5, s=25, color='steelblue', edgecolors='white', linewidths=0.3)\nlims = [0, max(yr_test_raw.max(), y_pred_r.max())]\naxes[0,1].plot(lims, lims, 'r--', lw=1.5)\naxes[0,1].set_xlabel('Actual reactions'); axes[0,1].set_ylabel('Predicted reactions')\naxes[0,1].set_title(f'{best_r_name}: Reactions (original scale)', fontweight='bold')\n\n# Log scale — comments\naxes[1,0].scatter(yc_test, y_pred_c_log, alpha=0.5, s=25, color='darkorange', edgecolors='white', linewidths=0.3)\nlims = [min(yc_test.min(), y_pred_c_log.min()), max(yc_test.max(), y_pred_c_log.max())]\naxes[1,0].plot(lims, lims, 'r--', lw=1.5)\naxes[1,0].set_xlabel('Actual log(1+comments)'); axes[1,0].set_ylabel('Predicted log(1+comments)')\naxes[1,0].set_title(f'{best_c_name}: Comments (log scale)', fontweight='bold')\n\n# Original scale — comments\naxes[1,1].scatter(yc_test_raw, y_pred_c, alpha=0.5, s=25, color='darkorange', edgecolors='white', linewidths=0.3)\nlims = [0, max(yc_test_raw.max(), y_pred_c.max())]\naxes[1,1].plot(lims, lims, 'r--', lw=1.5)\naxes[1,1].set_xlabel('Actual comments'); axes[1,1].set_ylabel('Predicted comments')\naxes[1,1].set_title(f'{best_c_name}: Comments (original scale)', fontweight='bold')\n\nplt.suptitle(f'{best_r_name} (reactions) / {best_c_name} (comments) — Predicted vs Actual',\n             fontsize=13, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "811e71f8",
   "metadata": {},
   "source": [
    "## Section 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9faafe",
   "metadata": {},
   "outputs": [],
   "source": "def plot_importance(model, feature_names, title, color='steelblue', top_n=20, X=None, y=None):\n    if hasattr(model, 'feature_importances_'):\n        imp = pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False).head(top_n)\n        xlabel = 'Mean Decrease in Impurity (MDI)'\n    else:\n        assert X is not None and y is not None, \"X and y required for permutation importance\"\n        print(f\"  Computing permutation importance for {title.split('—')[0].strip()}...\")\n        result = permutation_importance(model, X, y, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n        imp = pd.Series(result.importances_mean, index=feature_names).sort_values(ascending=False).head(top_n)\n        xlabel = 'Mean Decrease in R² (permutation, 10 repeats)'\n\n    fig, ax = plt.subplots(figsize=(10, 7))\n    imp_sorted = imp.sort_values()\n    bars = ax.barh(imp_sorted.index, imp_sorted.values, color=color, edgecolor='white')\n    ax.set_xlabel(xlabel)\n    ax.set_title(title, fontweight='bold', fontsize=12)\n    ax.set_xlim(0, imp_sorted.values.max() * 1.15)\n    for bar, v in zip(bars, imp_sorted.values):\n        ax.text(v + imp_sorted.values.max()*0.01, bar.get_y()+bar.get_height()/2,\n                f'{v:.4f}', va='center', fontsize=7)\n    plt.tight_layout()\n    plt.show()\n\nplot_importance(best_model_r, feature_names_r, f'{best_r_name} Feature Importance — Reactions',\n                color='steelblue',  X=X_test_r, y=yr_test)\nplot_importance(best_model_c, feature_names_c, f'{best_c_name} Feature Importance — Comments',\n                color='darkorange', X=X_test_c, y=yc_test)"
  },
  {
   "cell_type": "markdown",
   "id": "37fae2f5",
   "metadata": {},
   "source": [
    "## Section 8. Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0a83f",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\nplot_tree(dt, feature_names=feature_names_r, ax=axes[0],\n          filled=True, rounded=True, fontsize=7, max_depth=3,\n          class_names=None, impurity=False, precision=2)\naxes[0].set_title('Decision Tree — Reactions\\n(depth=4, msl=30, top 3 levels shown)',\n                   fontweight='bold', fontsize=10)\n\nplot_tree(dt_c, feature_names=feature_names_c, ax=axes[1],\n          filled=True, rounded=True, fontsize=7, max_depth=3,\n          class_names=None, impurity=False, precision=2)\naxes[1].set_title('Decision Tree — Comments\\n(depth=4, msl=30, top 3 levels shown)',\n                  fontweight='bold', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint('Decision Tree Rules (Reactions, max depth 4):')\nprint(export_text(dt, feature_names=feature_names_r, max_depth=4))"
  },
  {
   "cell_type": "markdown",
   "id": "edf02b25",
   "metadata": {},
   "source": [
    "## Section 9. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58474cb8",
   "metadata": {},
   "outputs": [],
   "source": "print('='*80)\nprint('FINAL RESULTS SUMMARY')\nprint('='*80)\n\ndef summary_df(results):\n    rows = []\n    for r in results:\n        rows.append({\n            'Model': r['label'],\n            'Log R²': round(r['log_r2'], 4),\n            'Log MAE': round(r['log_mae'], 4),\n            'MAE (orig)': round(r['mae'], 1),\n            'RMSE (orig)': round(r['rmse'], 1),\n            'R² (orig)': round(r['r2'], 4),\n            'sMAPE (%)': round(r['smape'], 1),\n            'MedAE (orig)': round(r['medae'], 1),\n            'Spearman ρ': round(r['spearman_rho'], 4),\n        })\n    return pd.DataFrame(rows).set_index('Model')\n\nprint(f'\\nFeatures used — Reactions: {len(feature_names_r)}  Comments: {len(feature_names_c)}')\n\nprint('\\nREACTIONS:')\ndf_r = summary_df(results_reactions)\nprint(df_r.to_string())\n\nprint('\\nCOMMENTS:')\ndf_c = summary_df(results_comments)\nprint(df_c.to_string())\n\nbest_r = df_r['Log R²'].idxmax()\nbest_c = df_c['Log R²'].idxmax()\nprint(f'\\nBest for reactions: {best_r}  (Log R²={df_r.loc[best_r, \"Log R²\"]}  Spearman ρ={df_r.loc[best_r, \"Spearman ρ\"]})')\nprint(f'Best for comments:  {best_c}  (Log R²={df_c.loc[best_c, \"Log R²\"]}  Spearman ρ={df_c.loc[best_c, \"Spearman ρ\"]})')"
  },
  {
   "cell_type": "markdown",
   "id": "cf6d9aaf",
   "metadata": {},
   "source": [
    "## Section 10. Predict on New Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e18b4c",
   "metadata": {},
   "outputs": [],
   "source": "def predict_engagement(\n    followers=10_000,\n    connections=500,\n    time_spent=30,\n    num_hashtags=3,\n    media_type='post',\n    content=\"Excited to share that I've just been promoted to Senior Engineer! This journey has been incredible.\",\n    headline='Senior Software Engineer',\n    num_content_links=0,\n):\n    \"\"\"\n    Predict reactions and comments for a new LinkedIn post.\n    Author baseline always uses the global LOO mean — author identity\n    is not available at inference time.\n    Each model uses its own LOO stats (reactions-based vs comments-based).\n    Sample index is set to -1 so it never collides with training-set indices\n    in the per-row LOO lookup, triggering the global-mean fallback correctly.\n    \"\"\"\n    sample = pd.DataFrame(\n        [{\n            'followers':     followers,\n            'connections':   connections,\n            'time_spent':    time_spent,\n            'num_hashtags':  num_hashtags,\n            'media_type':    media_type,\n            'content':       content,\n            'headline':      headline,\n            'content_links': np.nan if num_content_links == 0 else '; '.join(['http://example.com'] * num_content_links),\n            'hashtags':      np.nan,\n            'name':          '__unknown__',\n        }],\n        index=[-1]   # sentinel — not in any training-set LOO per-row dict\n    )\n\n    # Reactions model\n    X_new_r = engineer_features(sample, loo_stats_r).fillna(0)\n    for col in feature_names_r:\n        if col not in X_new_r.columns:\n            X_new_r[col] = 0\n    X_new_r = X_new_r[feature_names_r]\n\n    # Comments model\n    X_new_c = engineer_features(sample, loo_stats_c).fillna(0)\n    for col in feature_names_c:\n        if col not in X_new_c.columns:\n            X_new_c[col] = 0\n    X_new_c = X_new_c[feature_names_c]\n\n    pred_r_log = best_model_r.predict(X_new_r)[0]\n    pred_c_log = rf_plus_c.predict(X_new_c)[0]\n    pred_reactions = max(0, np.expm1(pred_r_log))\n    pred_comments  = max(0, np.expm1(pred_c_log))\n\n    print(f\"{followers:,} followers  |  {media_type}  |  {num_hashtags} hashtags\")\n    print(f\"  Predicted reactions: {pred_reactions:.0f}  (log={pred_r_log:.3f})\")\n    print(f\"  Predicted comments:  {pred_comments:.0f}   (log={pred_c_log:.3f})\")\n    return pred_reactions, pred_comments\n\nprint(f\"predict_engagement uses: reactions={best_r_name}  comments=RF+\\n\")\n\n# ── Example 1: Large account, personal vulnerability story ────────────────────\nprint(\"--- Example 1: Large account, personal story ---\")\npredict_engagement(\n    followers=150_000, num_hashtags=5, media_type='post',\n    content='After 10 years I finally failed. And that failure taught me everything. I was scared to share this but here it is...'\n)\n\n# ── Example 2: Mid-size account, article ──────────────────────────────────────\nprint(\"\\n--- Example 2: Mid-size account, article ---\")\npredict_engagement(\n    followers=25_000, num_hashtags=4, media_type='article',\n    content='How AI is transforming enterprise software. What every CTO needs to know about machine learning adoption in 2025.'\n)\n\n# ── Example 3: Small account, announcement ────────────────────────────────────\nprint(\"\\n--- Example 3: Small account, announcement post ---\")\npredict_engagement(\n    followers=8_000, num_hashtags=2, media_type='post',\n    content=\"Excited to share that I've just been promoted to Senior Engineer! This journey has been incredible.\"\n)"
  },
  {
   "cell_type": "markdown",
   "id": "c4z2b8c94op",
   "source": "## Section 11. Save HGBR Models for Streamlit",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dpgk8jwxn1u",
   "source": "import joblib\nimport json\nimport os\nfrom datetime import datetime\n\n# ── Save directory ─────────────────────────────────────────────────────────────\nSAVE_DIR = '../models/hgbr_streamlit'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# ── 1. Save HGBR models ────────────────────────────────────────────────────────\njoblib.dump(hgbr,   f'{SAVE_DIR}/hgbr_reactions.pkl')\njoblib.dump(hgbr_c, f'{SAVE_DIR}/hgbr_comments.pkl')\nprint(f'✓ hgbr_reactions.pkl  (MAE loss, lr=0.05, iter=500)')\nprint(f'✓ hgbr_comments.pkl   (MAE loss, lr=0.05, iter=500)')\n\n# ── 2. Save feature name lists ─────────────────────────────────────────────────\nwith open(f'{SAVE_DIR}/feature_names_reactions.json', 'w') as f:\n    json.dump(feature_names_r, f, indent=2)\n\nwith open(f'{SAVE_DIR}/feature_names_comments.json', 'w') as f:\n    json.dump(feature_names_c, f, indent=2)\n\nprint(f'✓ feature_names_reactions.json  ({len(feature_names_r)} features)')\nprint(f'✓ feature_names_comments.json   ({len(feature_names_c)} features)')\n\n# ── 3. Save LOO author stats (needed to score new posts at inference time) ──────\n# Convert numpy scalars / arrays to plain Python so json.dump works\ndef _loo_to_serialisable(stats):\n    return {\n        'loo_log_mean':   {k: float(v) for k, v in stats['loo_log_mean'].items()},\n        'loo_log_median': {k: float(v) for k, v in stats['loo_log_median'].items()},\n        'post_count':     {k: int(v)   for k, v in stats['post_count'].items()},\n        'global_log_mean':   float(stats['global_log_mean']),\n        'global_log_median': float(stats['global_log_median']),\n        # per-row dicts have integer keys (df index); store as str for JSON\n        'loo_log_mean_per_row':   {str(k): float(v) for k, v in stats['loo_log_mean_per_row'].items()},\n        'loo_log_median_per_row': {str(k): float(v) for k, v in stats['loo_log_median_per_row'].items()},\n    }\n\nwith open(f'{SAVE_DIR}/loo_stats_reactions.json', 'w') as f:\n    json.dump(_loo_to_serialisable(loo_stats_r), f, indent=2)\n\nwith open(f'{SAVE_DIR}/loo_stats_comments.json', 'w') as f:\n    json.dump(_loo_to_serialisable(loo_stats_c), f, indent=2)\n\nprint(f'✓ loo_stats_reactions.json')\nprint(f'✓ loo_stats_comments.json')\n\n# ── 4. Save model metadata ─────────────────────────────────────────────────────\n# Re-evaluate on held-out test set to record metrics alongside the artefacts\nhgbr_metrics_r = evaluate(yr_test, hgbr.predict(X_test_r),   yr_test_raw, 'HGBR reactions')\nhgbr_metrics_c = evaluate(yc_test, hgbr_c.predict(X_test_c), yc_test_raw, 'HGBR comments')\n\nmetadata = {\n    'saved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'model_type': 'HistGradientBoostingRegressor',\n    'target_transform': 'log1p (log(1+y)); predictions back-transformed with expm1',\n    'reactions': {\n        'file': 'hgbr_reactions.pkl',\n        'params': {'loss': 'absolute_error', 'learning_rate': 0.05, 'max_iter': 500,\n                   'max_depth': 6, 'min_samples_leaf': 20, 'l2_regularization': 0.1},\n        'n_features': len(feature_names_r),\n        'feature_names_file': 'feature_names_reactions.json',\n        'loo_stats_file': 'loo_stats_reactions.json',\n        'test_metrics': {k: (float(v) if isinstance(v, (np.floating, float)) else v)\n                         for k, v in hgbr_metrics_r.items() if k != 'label'},\n    },\n    'comments': {\n        'file': 'hgbr_comments.pkl',\n        'params': {'loss': 'absolute_error', 'learning_rate': 0.05, 'max_iter': 500,\n                   'max_depth': 6, 'min_samples_leaf': 20, 'l2_regularization': 0.1},\n        'n_features': len(feature_names_c),\n        'feature_names_file': 'feature_names_comments.json',\n        'loo_stats_file': 'loo_stats_comments.json',\n        'test_metrics': {k: (float(v) if isinstance(v, (np.floating, float)) else v)\n                         for k, v in hgbr_metrics_c.items() if k != 'label'},\n    },\n}\n\nwith open(f'{SAVE_DIR}/metadata.json', 'w') as f:\n    json.dump(metadata, f, indent=2)\n\nprint(f'✓ metadata.json')\n\nprint(f'\\n{\"=\"*60}')\nprint(f'All artefacts saved to: {SAVE_DIR}/')\nprint(f'{\"=\"*60}')\nprint(f'\\nReactions HGBR  — Log R²: {hgbr_metrics_r[\"log_r2\"]:.4f}  MAE: {hgbr_metrics_r[\"mae\"]:.1f}  sMAPE: {hgbr_metrics_r[\"smape\"]:.1f}%')\nprint(f'Comments  HGBR  — Log R²: {hgbr_metrics_c[\"log_r2\"]:.4f}  MAE: {hgbr_metrics_c[\"mae\"]:.1f}  sMAPE: {hgbr_metrics_c[\"smape\"]:.1f}%')\nprint(f'\\nStreamlit usage:')\nprint(f'  import joblib, json')\nprint(f'  hgbr_r = joblib.load(\"{SAVE_DIR}/hgbr_reactions.pkl\")')\nprint(f'  hgbr_c = joblib.load(\"{SAVE_DIR}/hgbr_comments.pkl\")')\nprint(f'  feat_r = json.load(open(\"{SAVE_DIR}/feature_names_reactions.json\"))')\nprint(f'  feat_c = json.load(open(\"{SAVE_DIR}/feature_names_comments.json\"))')\nprint(f'  loo_r  = json.load(open(\"{SAVE_DIR}/loo_stats_reactions.json\"))')\nprint(f'  loo_c  = json.load(open(\"{SAVE_DIR}/loo_stats_comments.json\"))')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}