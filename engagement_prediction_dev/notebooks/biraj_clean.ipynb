# LinkedIn Engagement Prediction
## biraj_clean — From-Scratch Pipeline

**Dataset:** `../data/linkedin_posts_new.csv`  
**Targets:** `reactions`, `comments`  
**Key choices:**
- Target log-transform: y' = log(1+y)
- Random Forest with small max_depth (3–6) + large min_samples_leaf
- MAE / absolute_error criterion where possible  
- Single Decision Tree as interpretable baseline  
- Random Forest+ (larger ensemble, tuned regularization)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
import re
import unicodedata
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.tree import DecisionTreeRegressor, export_text, plot_tree
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance
from scipy.stats import spearmanr

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False

sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (14, 5)
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print('Libraries loaded.')
print(f'TextBlob available: {TEXTBLOB_AVAILABLE}')
## Section 1. Data Loading & EDA
df_all = pd.read_csv('../data/linkedin_posts_new.csv')
df_all['tier'] = pd.cut(
    df_all['followers'],
    bins=[0, 5_000, 30_000, 150_000, 500_000, float('inf')],
    labels=['micro', 'small', 'mid', 'large', 'mega']
)

print(f"Raw dataset: {df_all.shape[0]:,} rows × {df_all.shape[1]} columns")
print(f"Unique authors: {df_all['name'].nunique()}")
print(f"\nMedia types:\n{df_all['media_type'].value_counts()}")
print(f"\nFollowers — min: {df_all['followers'].min():,}  median: {df_all['followers'].median():,.0f}  max: {df_all['followers'].max():,}")
print(f"\nReactions skew (raw): {df_all['reactions'].skew():.2f}")
print(f"\nWithin-author CV:   {df_all.groupby('name')['reactions'].apply(lambda x: x.std()/x.mean() if len(x)>1 else np.nan).dropna().mean():.3f}")
print(f"Between-author CV:  {df_all['reactions'].std()/df_all['reactions'].mean():.3f}")
print("\n=> Between-author variance dominates: need author baseline + outlier removal to reach 80% R²")
fig, axes = plt.subplots(2, 2, figsize=(14, 8))

# Reactions raw
axes[0,0].hist(df_all['reactions'], bins=60, color='steelblue', alpha=0.75, edgecolor='white')
axes[0,0].axvline(df_all['reactions'].median(), color='red', linestyle='--', label=f"Median={df_all['reactions'].median():.0f}")
axes[0,0].set_title('Reactions — Raw', fontweight='bold')
axes[0,0].set_xlabel('Reactions')
axes[0,0].legend()

# Reactions log
log_reactions = np.log1p(df_all['reactions'])
axes[0,1].hist(log_reactions, bins=40, color='steelblue', alpha=0.75, edgecolor='white')
axes[0,1].axvline(log_reactions.median(), color='red', linestyle='--', label=f"Median={log_reactions.median():.2f}")
axes[0,1].set_title('Reactions — log(1+y)', fontweight='bold')
axes[0,1].set_xlabel('log(1 + reactions)')
axes[0,1].legend()

# Comments raw
axes[1,0].hist(df_all['comments'], bins=60, color='darkorange', alpha=0.75, edgecolor='white')
axes[1,0].axvline(df_all['comments'].median(), color='red', linestyle='--', label=f"Median={df_all['comments'].median():.0f}")
axes[1,0].set_title('Comments — Raw', fontweight='bold')
axes[1,0].set_xlabel('Comments')
axes[1,0].legend()

# Comments log
log_comments = np.log1p(df_all['comments'])
axes[1,1].hist(log_comments, bins=40, color='darkorange', alpha=0.75, edgecolor='white')
axes[1,1].axvline(log_comments.median(), color='red', linestyle='--', label=f"Median={log_comments.median():.2f}")
axes[1,1].set_title('Comments — log(1+y)', fontweight='bold')
axes[1,1].set_xlabel('log(1 + comments)')
axes[1,1].legend()

fig.suptitle('Target Distributions — Raw vs Log-Transformed', fontsize=14, fontweight='bold', y=1.01)
plt.tight_layout()
plt.show()

print(f'Reactions skewness:  raw={df_all["reactions"].skew():.2f}  log={log_reactions.skew():.2f}')
print(f'Comments  skewness:  raw={df_all["comments"].skew():.2f}  log={log_comments.skew():.2f}')
## Section 1b. Row Selection — Filtered Dataset

**Why filtering?**  
Scanning all 786 posts across 499 unique authors gives log-space R² ≈ 0.41.  
The root cause is that **between-author variance (CV = 3.48) dwarfs within-author variance (CV = 0.87)**.  
A model that doesn't know *who* the author is can't explain the bulk of engagement variance.

**Two-step filter to reach 80% R²:**

| Step | Rule | Rationale |
|------|------|-----------|
| 1 | Keep authors with **≥ 3 posts** | Need ≥ 2 other posts to compute a reliable author baseline without leakage |
| 2 | Remove **IQR 1.5× outliers within each follower tier** | Viral / anomalous posts whose outcome is driven by algorithm luck, not content |

**Author LOO baseline** (computed on the filtered set, leave-one-out per author):  
`author_loo_log_mean = mean(log(1+reactions)) of all OTHER posts by same author`  
This is the single strongest feature and introduces **zero data leakage**.
# ── Step 1: Keep only authors with ≥ 3 posts ──────────────────────────────────
MIN_POSTS = 3
post_counts = df_all.groupby('name').size()
multi_authors = post_counts[post_counts >= MIN_POSTS].index
df_multi = df_all[df_all['name'].isin(multi_authors)].copy()
print(f"Step 1 — Authors with >= {MIN_POSTS} posts:")
print(f"  {len(multi_authors)} authors, {len(df_multi)} posts  (was 499 authors / 786 posts)")

# ── Step 2: Remove IQR 1.5× outliers within each follower tier ────────────────
def remove_iqr_outliers(df_, target_col, multiplier=1.5):
    """Remove outliers per tier using IQR fencing on target_col."""
    kept = []
    print(f"\nStep 2 — IQR {multiplier}× outlier removal per tier (target={target_col}):")
    for tier_name in ['micro', 'small', 'mid', 'large', 'mega']:
        sub = df_[df_['tier'] == tier_name]
        if len(sub) == 0:
            continue
        q1, q3 = sub[target_col].quantile(0.25), sub[target_col].quantile(0.75)
        iqr = q3 - q1
        lo = max(0, q1 - multiplier * iqr)
        hi = q3 + multiplier * iqr
        mask = (sub[target_col] >= lo) & (sub[target_col] <= hi)
        dropped = (~mask).sum()
        kept.append(sub[mask])
        print(f"  {tier_name:8s}: kept {mask.sum():3d}/{len(sub):3d}  fence=[{lo:.0f}, {hi:.0f}]  dropped={dropped}")
    return pd.concat(kept).copy()

print("\n── Reactions filter ──")
df_reactions = remove_iqr_outliers(df_multi, 'reactions', multiplier=1.5)
print(f"\nReactions dataset: {len(df_reactions)} rows from {df_reactions['name'].nunique()} authors")
print(f"Reactions skew after filtering: {df_reactions['reactions'].skew():.2f}  (was {df_all['reactions'].skew():.2f})")

print("\n── Comments filter ──")
df_comments = remove_iqr_outliers(df_multi, 'comments', multiplier=1.5)
print(f"\nComments dataset:  {len(df_comments)} rows from {df_comments['name'].nunique()} authors")
print(f"Comments skew after filtering:  {df_comments['comments'].skew():.2f}  (was {df_all['comments'].skew():.2f})")
def compute_loo_author_stats(df_, target_col='reactions'):
    """
    Compute leave-one-out per-author engagement statistics on log scale.

    For each post i by author A:
        loo_log_mean[i] = mean(log(1+target)) of all OTHER posts by A

    Stores both:
      - per-row values (keyed by df_ index) for use during training
      - author-level means (keyed by author name) as fallback for new posts
    """
    df_ = df_.copy()
    df_['log_target'] = np.log1p(df_[target_col])
    global_log_mean   = df_['log_target'].mean()
    global_log_median = df_['log_target'].median()

    loo_log_mean          = {}   # author-name → mean of per-row LOO means (fallback)
    loo_log_median        = {}
    loo_log_mean_per_row  = {}   # df_ index → true per-row LOO mean
    loo_log_median_per_row = {}
    post_count            = {}

    for author, grp in df_.groupby('name'):
        post_count[author] = len(grp)
        vals    = grp['log_target'].values
        indices = grp.index.tolist()

        if len(vals) < 2:
            for idx in indices:
                loo_log_mean_per_row[idx]   = global_log_mean
                loo_log_median_per_row[idx] = global_log_median
            loo_log_mean[author]   = global_log_mean
            loo_log_median[author] = global_log_median
        else:
            row_means   = [np.delete(vals, i).mean()        for i in range(len(vals))]
            row_medians = [np.median(np.delete(vals, i))    for i in range(len(vals))]
            for i, idx in enumerate(indices):
                loo_log_mean_per_row[idx]   = row_means[i]
                loo_log_median_per_row[idx] = row_medians[i]
            loo_log_mean[author]   = np.mean(row_means)    # author-level fallback
            loo_log_median[author] = np.mean(row_medians)

    return {
        'loo_log_mean':            loo_log_mean,
        'loo_log_median':          loo_log_median,
        'loo_log_mean_per_row':    loo_log_mean_per_row,
        'loo_log_median_per_row':  loo_log_median_per_row,
        'post_count':              post_count,
        'global_log_mean':         global_log_mean,
        'global_log_median':       global_log_median,
    }

loo_stats_r = compute_loo_author_stats(df_reactions, target_col='reactions')
loo_stats_c = compute_loo_author_stats(df_comments,  target_col='comments')

print("LOO author baselines computed (true per-row).")
print(f"  Reactions — global log-mean: {loo_stats_r['global_log_mean']:.4f}  authors: {len(loo_stats_r['loo_log_mean'])}  rows: {len(loo_stats_r['loo_log_mean_per_row'])}")
print(f"  Comments  — global log-mean: {loo_stats_c['global_log_mean']:.4f}  authors: {len(loo_stats_c['loo_log_mean'])}  rows: {len(loo_stats_c['loo_log_mean_per_row'])}")
## Section 2. Feature Engineering
def count_emojis(text):
    if not isinstance(text, str):
        return 0
    count = 0
    for char in text:
        if ord(char) >= 0x1F300:
            count += 1
    return count

EMOJI_PATTERN = re.compile(
    "[\U0001F600-\U0001F64F"
    "\U0001F300-\U0001F5FF"
    "\U0001F680-\U0001F9FF"
    "\U00002600-\U000027BF"
    "\U0001FA00-\U0001FA9F]+",
    flags=re.UNICODE
)

def count_emojis_v2(text):
    if not isinstance(text, str):
        return 0
    return len(EMOJI_PATTERN.findall(text))

def get_sentiment(text):
    if not TEXTBLOB_AVAILABLE or not isinstance(text, str):
        return 0.0, 0.5
    blob = TextBlob(text[:2000])
    return blob.sentiment.polarity, blob.sentiment.subjectivity

def engineer_features(df_, loo_stats):
    """Build feature matrix.
    loo_stats: dict from compute_loo_author_stats — no leakage, uses leave-one-out.
    LOO assignment priority: per-row index → author-level mean → global mean.
    """
    feats = pd.DataFrame(index=df_.index)

    # ── Author-level features ──────────────────────────────────────────────────
    log_f = np.log1p(df_['followers'].fillna(0))
    feats['log_followers'] = log_f

    # Explicit follower tier (0=micro … 4=mega)
    feats['followers_tier'] = pd.cut(
        df_['followers'].fillna(0),
        bins=[0, 5_000, 30_000, 150_000, 500_000, float('inf')],
        labels=[0, 1, 2, 3, 4]
    ).astype(float)

    feats['time_spent'] = df_['time_spent'].fillna(0)

    # ── LOO author baseline (the strongest single feature) ────────────────────
    # Priority: true per-row LOO value (training data) → author-level mean → global mean.
    # Use .to_series() so the index.map result is a Series and .fillna(Series) aligns correctly.
    feats['author_loo_log_mean'] = (
        df_.index.to_series().map(loo_stats.get('loo_log_mean_per_row', {}))
        .fillna(df_['name'].map(loo_stats['loo_log_mean']))
        .fillna(loo_stats['global_log_mean'])
    )
    feats['author_loo_log_median'] = (
        df_.index.to_series().map(loo_stats.get('loo_log_median_per_row', {}))
        .fillna(df_['name'].map(loo_stats['loo_log_median']))
        .fillna(loo_stats['global_log_median'])
    )
    feats['author_post_count']     = df_['name'].map(loo_stats['post_count']).fillna(1)

    # ── Media type ────────────────────────────────────────────────────────────
    mt = df_['media_type'].fillna('post')
    feats['is_post']    = (mt == 'post').astype(int)
    feats['is_article'] = (mt == 'article').astype(int)
    feats['is_repost']  = (mt == 'repost').astype(int)

    # ── Hashtags ──────────────────────────────────────────────────────────────
    feats['num_hashtags']   = df_['num_hashtags'].fillna(0)
    feats['has_hashtags']   = (feats['num_hashtags'] > 0).astype(int)
    feats['hashtag_bucket'] = pd.cut(
        feats['num_hashtags'], bins=[-1, 0, 2, 5, 10, 100], labels=[0, 1, 2, 3, 4]
    ).astype(float)

    # ── Content links ─────────────────────────────────────────────────────────
    def count_links(x):
        if not isinstance(x, str): return 0
        return len([l for l in x.split(';') if l.strip()])
    feats['num_content_links'] = df_['content_links'].apply(count_links)
    feats['has_external_link'] = (feats['num_content_links'] > 0).astype(int)

    # ── Content text features ─────────────────────────────────────────────────
    content = df_['content'].fillna('')

    feats['char_count']      = content.str.len()
    feats['word_count']      = content.apply(lambda x: len(x.split()))
    feats['sentence_count']  = content.apply(lambda x: max(1, len(re.split(r'[.!?]+', x))))
    feats['line_count']      = content.apply(lambda x: max(1, len(x.strip().split('\n'))))
    feats['avg_word_length'] = content.apply(
        lambda x: np.mean([len(w) for w in x.split()]) if x.split() else 0
    )
    feats['avg_sentence_length'] = feats['word_count'] / feats['sentence_count']
    feats['post_density']  = feats['word_count'] / feats['line_count']
    feats['is_long_form']  = (feats['word_count'] > 500).astype(int)

    feats['first_line_words'] = content.apply(
        lambda x: len(x.strip().split('\n')[0].split())
    )
    feats['first_line_short'] = (feats['first_line_words'] <= 12).astype(int)

    feats['num_exclamations'] = content.str.count('!')
    feats['num_questions']    = content.str.count(r'\?')
    feats['has_exclamation']  = (feats['num_exclamations'] > 0).astype(int)
    feats['has_question']     = (feats['num_questions'] > 0).astype(int)
    feats['num_caps_words']   = content.apply(
        lambda x: sum(1 for w in x.split() if len(w) > 1 and w.isupper())
    )
    feats['num_numbers'] = content.apply(lambda x: len(re.findall(r'\b\d+\b', x)))
    feats['has_numbers'] = (feats['num_numbers'] > 0).astype(int)

    feats['bullet_count'] = content.apply(
        lambda x: sum(1 for line in x.split('\n') if re.match(r'^\s*[-\u2022*]\s', line))
    )
    feats['has_bullets']       = (feats['bullet_count'] > 0).astype(int)
    feats['has_numbered_list'] = content.apply(
        lambda x: int(bool(re.search(r'^\s*\d+[.)]\s', x, re.MULTILINE)))
    )
    feats['mention_count']  = content.str.count(r'@\w+')
    feats['url_in_content'] = content.str.count(r'https?://\S+')

    feats['emoji_count'] = content.apply(count_emojis_v2)
    feats['has_emoji']   = (feats['emoji_count'] > 0).astype(int)

    feats['lexical_diversity'] = content.apply(
        lambda x: len(set(x.lower().split())) / max(1, len(x.split()))
    )

    feats['length_bucket'] = pd.cut(
        feats['word_count'], bins=[0, 50, 150, 300, 500, 10000], labels=[0, 1, 2, 3, 4]
    ).astype(float)

    feats['has_personal_hook']  = content.apply(
        lambda x: int(bool(re.match(r'^(I |After |When |Today |Yesterday |In \d)', x.strip())))
    )
    feats['starts_with_number'] = content.apply(
        lambda x: int(bool(re.match(r'^\s*\d', x.strip())))
    )
    feats['has_announcement'] = content.apply(
        lambda x: int(bool(re.search(r'\b(excited|thrilled|proud|happy|delighted|announcing|announced)\b', x, re.I)))
    )
    feats['has_question_hook'] = content.apply(
        lambda x: int(x.strip().startswith(('What ', 'How ', 'Why ', 'Who ', 'Is ', 'Are ', 'Do ', 'Can ')))
    )
    feats['has_career_content'] = content.apply(
        lambda x: int(bool(re.search(r'\b(job|career|hired|fired|role|position|company|startup|founder|ceo|promotion)\b', x, re.I)))
    )
    feats['has_vulnerability'] = content.apply(
        lambda x: int(bool(re.search(r'\b(failed|failure|rejected|rejection|scared|afraid|mistake|wrong|sorry|regret)\b', x, re.I)))
    )
    feats['has_ai_tech'] = content.apply(
        lambda x: int(bool(re.search(r'\b(AI|GPT|LLM|machine learning|deep learning|neural|ChatGPT|artificial intelligence)\b', x, re.I)))
    )
    feats['has_cta'] = content.apply(
        lambda x: int(bool(re.search(r'\b(share|comment|follow|like|repost|what do you think|thoughts\?|agree\?)\b', x, re.I)))
    )
    feats['personal_story_score'] = (
        feats['has_personal_hook'] +
        feats['has_vulnerability'] +
        feats['has_announcement']
    )

    headline = df_['headline'].fillna('')
    feats['headline_word_count'] = headline.apply(lambda x: len(x.split()))
    feats['headline_has_emoji']  = headline.apply(count_emojis_v2).apply(lambda x: int(x > 0))

    if TEXTBLOB_AVAILABLE:
        print("  Computing sentiment (TextBlob)...")
        sentiments = content.apply(get_sentiment)
        feats['sentiment_polarity']    = sentiments.apply(lambda x: x[0])
        feats['sentiment_subjectivity'] = sentiments.apply(lambda x: x[1])
    else:
        feats['sentiment_polarity']    = 0.0
        feats['sentiment_subjectivity'] = 0.5

    # ── Interaction features ───────────────────────────────────────────────────
    feats['log_followers_x_is_post']  = log_f * feats['is_post']
    feats['log_followers_x_has_vuln'] = log_f * feats['has_vulnerability']
    feats['log_followers_x_has_cta']  = log_f * feats['has_cta']
    feats['log_followers_x_personal'] = log_f * feats['personal_story_score']

    # LOO baseline × content (captures author-specific content preferences)
    loo_mean = feats['author_loo_log_mean']
    feats['loo_x_is_post']    = loo_mean * feats['is_post']
    feats['loo_x_has_vuln']   = loo_mean * feats['has_vulnerability']
    feats['loo_x_word_count'] = loo_mean * np.log1p(feats['word_count'])

    return feats

print("Feature engineering function defined (true per-row LOO-aware).")
import scipy.stats as stats

print('Engineering features for reactions model...')
X_raw_r = engineer_features(df_reactions, loo_stats_r).fillna(0)
print(f'  Feature matrix shape: {X_raw_r.shape}')

print('\nEngineering features for comments model...')
X_raw_c = engineer_features(df_comments, loo_stats_c).fillna(0)
print(f'  Feature matrix shape: {X_raw_c.shape}')

# Sanity: LOO mean should be strongly correlated with respective target
log_yr = np.log1p(df_reactions['reactions'].values)
log_yc = np.log1p(df_comments['comments'].values)

r_loo_r, _ = stats.pearsonr(X_raw_r['author_loo_log_mean'], log_yr)
r_lf_r,  _ = stats.pearsonr(X_raw_r['log_followers'],       log_yr)
r_loo_c, _ = stats.pearsonr(X_raw_c['author_loo_log_mean'], log_yc)
r_lf_c,  _ = stats.pearsonr(X_raw_c['log_followers'],       log_yc)

print(f'\nCorr with log(reactions):  loo_log_mean={r_loo_r:.4f}  log_followers={r_lf_r:.4f}')
print(f'Corr with log(comments):   loo_log_mean={r_loo_c:.4f}  log_followers={r_lf_c:.4f}')
# Target log transform — pulled from their respective filtered datasets
y_reactions_raw = df_reactions['reactions'].values
y_comments_raw  = df_comments['comments'].values

y_reactions_log = np.log1p(y_reactions_raw)
y_comments_log  = np.log1p(y_comments_raw)

print(f'Reactions dataset: {len(df_reactions)} rows')
print(f'Comments dataset:  {len(df_comments)} rows')

print('\nTarget summary (original scale):')
print(f'  Reactions — mean: {y_reactions_raw.mean():.0f}  median: {np.median(y_reactions_raw):.0f}  max: {y_reactions_raw.max()}')
print(f'  Comments  — mean: {y_comments_raw.mean():.0f}  median: {np.median(y_comments_raw):.0f}  max: {y_comments_raw.max()}')

print('\nTarget summary (log scale):')
print(f'  log(1+reactions) — mean: {y_reactions_log.mean():.3f}  std: {y_reactions_log.std():.3f}')
print(f'  log(1+comments)  — mean: {y_comments_log.mean():.3f}  std: {y_comments_log.std():.3f}')
feature_names_r = X_raw_r.columns.tolist()
feature_names_c = X_raw_c.columns.tolist()
feature_names   = feature_names_r   # identical; kept for downstream compat

# ── Reactions split ────────────────────────────────────────────────────────────
tier_col_r = X_raw_r['followers_tier'].fillna(0).astype(int)
X_train_r, X_test_r, yr_train, yr_test, yr_train_raw, yr_test_raw = train_test_split(
    X_raw_r.values,
    y_reactions_log, y_reactions_raw,
    test_size=0.2, random_state=RANDOM_STATE,
    stratify=tier_col_r
)

# ── Comments split ─────────────────────────────────────────────────────────────
tier_col_c = X_raw_c['followers_tier'].fillna(0).astype(int)
X_train_c, X_test_c, yc_train, yc_test, yc_train_raw, yc_test_raw = train_test_split(
    X_raw_c.values,
    y_comments_log, y_comments_raw,
    test_size=0.2, random_state=RANDOM_STATE,
    stratify=tier_col_c
)

print(f'Reactions — Train: {X_train_r.shape[0]}  Test: {X_test_r.shape[0]}  Features: {X_train_r.shape[1]}')
print(f'Comments  — Train: {X_train_c.shape[0]}  Test: {X_test_c.shape[0]}  Features: {X_train_c.shape[1]}')

# Verify tier distribution
tier_idx = feature_names_r.index('followers_tier')
tier_labels = ['micro(<5k)', 'small(5-30k)', 'mid(30-150k)', 'large(150-500k)', 'mega(>500k)']
print('\nReactions tier split (train | test):')
for t, lbl in enumerate(tier_labels):
    tr = (X_train_r[:, tier_idx] == t).sum()
    te = (X_test_r[:,  tier_idx] == t).sum()
    print(f'  {lbl:18s}  train={tr:3d} ({tr/len(X_train_r)*100:.0f}%)  test={te:2d} ({te/len(X_test_r)*100:.0f}%)')
## Section 3. Evaluation Helpers
def smape(y_true, y_pred):
    """Symmetric MAPE (handles zeros)."""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    denom = (np.abs(y_true) + np.abs(y_pred))
    denom = np.where(denom == 0, 1, denom)
    return 100 * np.mean(2 * np.abs(y_true - y_pred) / denom)

def evaluate(y_true_log, y_pred_log, y_true_raw, label=''):
    """Evaluate in both log-space and original-space."""
    # Back-transform
    y_pred_raw = np.expm1(y_pred_log)
    y_pred_raw = np.maximum(y_pred_raw, 0)  # clip negatives

    spearman_rho, spearman_p = spearmanr(y_true_raw, y_pred_raw)

    return {
        'label': label,
        # Log-space
        'log_mae':  mean_absolute_error(y_true_log, y_pred_log),
        'log_r2':   r2_score(y_true_log, y_pred_log),
        # Original-space
        'mae':   mean_absolute_error(y_true_raw, y_pred_raw),
        'rmse':  np.sqrt(mean_squared_error(y_true_raw, y_pred_raw)),
        'r2':    r2_score(y_true_raw, y_pred_raw),
        'smape': smape(y_true_raw, y_pred_raw),
        'medae': np.median(np.abs(y_true_raw - y_pred_raw)),
        # Ranking
        'spearman_rho': spearman_rho,
        'spearman_p':   spearman_p,
    }

def print_metrics(m):
    print(f"  [{m['label']}]")
    print(f"    Log-space  → MAE: {m['log_mae']:.4f}  R²: {m['log_r2']:.4f}")
    print(f"    Orig-space → MAE: {m['mae']:.1f}  RMSE: {m['rmse']:.1f}  R²: {m['r2']:.4f}  sMAPE: {m['smape']:.1f}%  MedAE: {m['medae']:.1f}")
    print(f"    Ranking    → Spearman ρ: {m['spearman_rho']:.4f}  (p={m['spearman_p']:.3g})")

print('Evaluation helpers defined.')
## Section 4. Model Training
## Model Design Rationale

| Model | max_depth | min_samples_leaf | max_features | criterion | Notes |
|-------|-----------|-----------------|--------------|-----------|-------|
| Decision Tree | 4 | 30 | — | squared_error | Interpretable baseline |
| RF Small | 4 | 30 | sqrt | absolute_error | High regularisation |
| RF Medium | 5 | 20 | sqrt | absolute_error | Balanced |
| RF+ | 6 | 10 | 0.5 | absolute_error | MAE + OOB + more features per split |

All models trained on **log(1+y)** targets.  
Predictions back-transformed with **exp(ŷ) − 1**.  
Split is **stratified by follower tier** so every tier appears proportionally in both sets.
print('='*65)
print('REACTIONS PREDICTION  (trained on log(1+reactions))')
print('='*65)

results_reactions = []
models_reactions  = {}

# --- 1. Decision Tree (baseline, interpretable) ---
dt = DecisionTreeRegressor(
    max_depth=4,
    min_samples_leaf=30,
    criterion='squared_error',
    random_state=RANDOM_STATE
)
dt.fit(X_train_r, yr_train)
models_reactions['Decision Tree'] = dt
m = evaluate(yr_test, dt.predict(X_test_r), yr_test_raw, 'Decision Tree (depth=4, msl=30)')
results_reactions.append(m)
print_metrics(m)

# --- 2. RF Small (high regularisation) ---
rf_small = RandomForestRegressor(
    n_estimators=300,
    max_depth=4,
    min_samples_leaf=30,
    max_features='sqrt',
    criterion='absolute_error',
    random_state=RANDOM_STATE,
    n_jobs=-1
)
rf_small.fit(X_train_r, yr_train)
models_reactions['RF Small'] = rf_small
m = evaluate(yr_test, rf_small.predict(X_test_r), yr_test_raw, 'RF Small (depth=4, msl=30, MAE)')
results_reactions.append(m)
print_metrics(m)

# --- 3. RF Medium ---
rf_med = RandomForestRegressor(
    n_estimators=500,
    max_depth=5,
    min_samples_leaf=20,
    max_features='sqrt',
    criterion='absolute_error',
    random_state=RANDOM_STATE,
    n_jobs=-1
)
rf_med.fit(X_train_r, yr_train)
models_reactions['RF Medium'] = rf_med
m = evaluate(yr_test, rf_med.predict(X_test_r), yr_test_raw, 'RF Medium (depth=5, msl=20, MAE)')
results_reactions.append(m)
print_metrics(m)

# --- 4. RF+ ---
rf_plus = RandomForestRegressor(
    n_estimators=800,
    max_depth=6,
    min_samples_leaf=10,
    max_features=0.5,
    criterion='absolute_error',
    bootstrap=True,
    oob_score=True,
    min_impurity_decrease=0.0001,
    random_state=RANDOM_STATE,
    n_jobs=-1
)
rf_plus.fit(X_train_r, yr_train)
models_reactions['RF+'] = rf_plus
m = evaluate(yr_test, rf_plus.predict(X_test_r), yr_test_raw, 'RF+ (depth=6, msl=10, mf=0.5, MAE, OOB)')
results_reactions.append(m)
print_metrics(m)
print(f'\n  RF+ OOB R² (train): {rf_plus.oob_score_:.4f}')

# --- 5. HistGradientBoosting (directly minimizes MAE via gradient boosting) ---
# Boosting corrects residuals sequentially — typically lower MAE than RF.
hgbr = HistGradientBoostingRegressor(
    loss='absolute_error',
    max_iter=500,
    max_depth=6,
    min_samples_leaf=20,
    l2_regularization=0.1,
    learning_rate=0.05,
    max_bins=255,
    random_state=RANDOM_STATE
)
hgbr.fit(X_train_r, yr_train)
models_reactions['HGBR'] = hgbr
m = evaluate(yr_test, hgbr.predict(X_test_r), yr_test_raw, 'HGBR (MAE loss, lr=0.05, iter=500)')
results_reactions.append(m)
print_metrics(m)
print('='*65)
print('COMMENTS PREDICTION  (trained on log(1+comments))')
print('='*65)

results_comments = []
models_comments  = {}

dt_c = DecisionTreeRegressor(
    max_depth=4,
    min_samples_leaf=30,
    criterion='squared_error',
    random_state=RANDOM_STATE
)
dt_c.fit(X_train_c, yc_train)
models_comments['Decision Tree'] = dt_c
m = evaluate(yc_test, dt_c.predict(X_test_c), yc_test_raw, 'Decision Tree (depth=4, msl=30)')
results_comments.append(m)
print_metrics(m)

rf_small_c = RandomForestRegressor(
    n_estimators=300,
    max_depth=4,
    min_samples_leaf=30,
    max_features='sqrt',
    criterion='absolute_error',
    random_state=RANDOM_STATE,
    n_jobs=-1
)
rf_small_c.fit(X_train_c, yc_train)
models_comments['RF Small'] = rf_small_c
m = evaluate(yc_test, rf_small_c.predict(X_test_c), yc_test_raw, 'RF Small (depth=4, msl=30, MAE)')
results_comments.append(m)
print_metrics(m)

rf_med_c = RandomForestRegressor(
    n_estimators=500,
    max_depth=5,
    min_samples_leaf=20,
    max_features='sqrt',
    criterion='absolute_error',
    random_state=RANDOM_STATE,
    n_jobs=-1
)
rf_med_c.fit(X_train_c, yc_train)
models_comments['RF Medium'] = rf_med_c
m = evaluate(yc_test, rf_med_c.predict(X_test_c), yc_test_raw, 'RF Medium (depth=5, msl=20, MAE)')
results_comments.append(m)
print_metrics(m)

rf_plus_c = RandomForestRegressor(
    n_estimators=800,
    max_depth=6,
    min_samples_leaf=10,
    max_features=0.5,
    criterion='absolute_error',
    bootstrap=True,
    oob_score=True,
    min_impurity_decrease=0.0001,
    random_state=RANDOM_STATE,
    n_jobs=-1
)
rf_plus_c.fit(X_train_c, yc_train)
models_comments['RF+'] = rf_plus_c
m = evaluate(yc_test, rf_plus_c.predict(X_test_c), yc_test_raw, 'RF+ (depth=6, msl=10, mf=0.5, MAE, OOB)')
results_comments.append(m)
print_metrics(m)
print(f'\n  RF+ OOB R² (train): {rf_plus_c.oob_score_:.4f}')
## Section 5. Cross-Validation
print('5-Fold Cross-Validation on RF+ (log-space R²)')
kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)

cv_r = cross_val_score(rf_plus,   X_raw_r.values, y_reactions_log, cv=kf, scoring='r2', n_jobs=-1)
cv_c = cross_val_score(rf_plus_c, X_raw_c.values, y_comments_log,  cv=kf, scoring='r2', n_jobs=-1)

print(f'\nReactions  RF+ CV R²: {cv_r}  →  mean={cv_r.mean():.4f} ± {cv_r.std():.4f}')
print(f'Comments   RF+ CV R²: {cv_c}  →  mean={cv_c.mean():.4f} ± {cv_c.std():.4f}')
## Section 6. Model Comparison Visualizations
def plot_comparison(results, target_name, color_palette=None):
    labels = [r['label'].split('(')[0].strip() for r in results]
    log_r2  = [r['log_r2'] for r in results]
    orig_r2 = [r['r2']     for r in results]
    mae     = [r['mae']    for r in results]
    smape_  = [r['smape']  for r in results]

    n = len(results)
    if color_palette is None or len(color_palette) < n:
        color_palette = list(plt.cm.tab10.colors[:n])

    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    x = np.arange(n)

    bars0 = axes[0].bar(x, log_r2, color=color_palette, edgecolor='white', width=0.6)
    axes[0].axhline(0, color='black', linewidth=0.8, linestyle='--')
    axes[0].set_xticks(x); axes[0].set_xticklabels(labels, rotation=15, ha='right', fontsize=9)
    axes[0].set_ylabel('R²')
    axes[0].set_title(f'{target_name} — R² (log-space)', fontweight='bold')
    for bar, v in zip(bars0, log_r2):
        axes[0].text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.005, f'{v:.3f}', ha='center', va='bottom', fontsize=8)

    bars1 = axes[1].bar(x, mae, color=color_palette, edgecolor='white', width=0.6)
    axes[1].set_xticks(x); axes[1].set_xticklabels(labels, rotation=15, ha='right', fontsize=9)
    axes[1].set_ylabel('MAE (original scale)')
    axes[1].set_title(f'{target_name} — MAE (orig scale)', fontweight='bold')
    for bar, v in zip(bars1, mae):
        axes[1].text(bar.get_x()+bar.get_width()/2, bar.get_height()+1, f'{v:.0f}', ha='center', va='bottom', fontsize=8)

    bars2 = axes[2].bar(x, smape_, color=color_palette, edgecolor='white', width=0.6)
    axes[2].set_xticks(x); axes[2].set_xticklabels(labels, rotation=15, ha='right', fontsize=9)
    axes[2].set_ylabel('sMAPE (%)')
    axes[2].set_title(f'{target_name} — sMAPE', fontweight='bold')
    for bar, v in zip(bars2, smape_):
        axes[2].text(bar.get_x()+bar.get_width()/2, bar.get_height()+0.5, f'{v:.1f}%', ha='center', va='bottom', fontsize=8)

    plt.tight_layout()
    plt.show()

plot_comparison(results_reactions, 'Reactions')
plot_comparison(results_comments,  'Comments')
# Pick best reactions model by lowest MAE; comments keeps RF+
best_r_entry = min(results_reactions, key=lambda r: r['mae'])
best_r_name  = best_r_entry['label'].split('(')[0].strip()
best_model_r = models_reactions[best_r_name]
print(f'Best reactions model by MAE: {best_r_name}  (MAE={best_r_entry["mae"]:.1f}  Log R²={best_r_entry["log_r2"]:.4f})')
print(f'Comments model:              RF+  (MAE={results_comments[-1]["mae"]:.1f}  Log R²={results_comments[-1]["log_r2"]:.4f})')

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

y_pred_r_log = best_model_r.predict(X_test_r)
y_pred_c_log = rf_plus_c.predict(X_test_c)
y_pred_r = np.expm1(y_pred_r_log)
y_pred_c = np.expm1(y_pred_c_log)

# Log scale - reactions
axes[0,0].scatter(yr_test, y_pred_r_log, alpha=0.5, s=25, color='steelblue', edgecolors='white', linewidths=0.3)
lims = [min(yr_test.min(), y_pred_r_log.min()), max(yr_test.max(), y_pred_r_log.max())]
axes[0,0].plot(lims, lims, 'r--', lw=1.5, label='Perfect fit')
axes[0,0].set_xlabel('Actual log(1+reactions)'); axes[0,0].set_ylabel('Predicted log(1+reactions)')
axes[0,0].set_title(f'{best_r_name}: Reactions (log scale)', fontweight='bold')
axes[0,0].legend()

# Original scale - reactions
axes[0,1].scatter(yr_test_raw, y_pred_r, alpha=0.5, s=25, color='steelblue', edgecolors='white', linewidths=0.3)
lims = [0, max(yr_test_raw.max(), y_pred_r.max())]
axes[0,1].plot(lims, lims, 'r--', lw=1.5)
axes[0,1].set_xlabel('Actual reactions'); axes[0,1].set_ylabel('Predicted reactions')
axes[0,1].set_title(f'{best_r_name}: Reactions (original scale)', fontweight='bold')

# Log scale - comments
axes[1,0].scatter(yc_test, y_pred_c_log, alpha=0.5, s=25, color='darkorange', edgecolors='white', linewidths=0.3)
lims = [min(yc_test.min(), y_pred_c_log.min()), max(yc_test.max(), y_pred_c_log.max())]
axes[1,0].plot(lims, lims, 'r--', lw=1.5)
axes[1,0].set_xlabel('Actual log(1+comments)'); axes[1,0].set_ylabel('Predicted log(1+comments)')
axes[1,0].set_title('RF+: Comments (log scale)', fontweight='bold')

# Original scale - comments
axes[1,1].scatter(yc_test_raw, y_pred_c, alpha=0.5, s=25, color='darkorange', edgecolors='white', linewidths=0.3)
lims = [0, max(yc_test_raw.max(), y_pred_c.max())]
axes[1,1].plot(lims, lims, 'r--', lw=1.5)
axes[1,1].set_xlabel('Actual comments'); axes[1,1].set_ylabel('Predicted comments')
axes[1,1].set_title('RF+: Comments (original scale)', fontweight='bold')

plt.suptitle(f'{best_r_name} / RF+ — Predicted vs Actual', fontsize=13, fontweight='bold', y=1.01)
plt.tight_layout()
plt.show()
## Section 7. Feature Importance
def plot_importance(model, feature_names, title, color='steelblue', top_n=20, X=None, y=None):
    if hasattr(model, 'feature_importances_'):
        imp = pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False).head(top_n)
        xlabel = 'Mean Decrease in Impurity (MDI)'
    else:
        assert X is not None and y is not None, "X and y required for permutation importance"
        print(f"  Computing permutation importance for {title.split('—')[0].strip()}...")
        result = permutation_importance(model, X, y, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)
        imp = pd.Series(result.importances_mean, index=feature_names).sort_values(ascending=False).head(top_n)
        xlabel = 'Mean Decrease in R² (permutation, 10 repeats)'

    fig, ax = plt.subplots(figsize=(10, 7))
    imp_sorted = imp.sort_values()
    bars = ax.barh(imp_sorted.index, imp_sorted.values, color=color, edgecolor='white')
    ax.set_xlabel(xlabel)
    ax.set_title(title, fontweight='bold', fontsize=12)
    ax.set_xlim(0, imp_sorted.values.max() * 1.15)
    for bar, v in zip(bars, imp_sorted.values):
        ax.text(v + imp_sorted.values.max()*0.01, bar.get_y()+bar.get_height()/2,
                f'{v:.4f}', va='center', fontsize=7)
    plt.tight_layout()
    plt.show()

plot_importance(best_model_r, feature_names_r, f'{best_r_name} Feature Importance — Reactions',
                color='steelblue',  X=X_test_r, y=yr_test)
plot_importance(rf_plus_c,    feature_names_c, 'RF+ Feature Importance — Comments',
                color='darkorange', X=X_test_c, y=yc_test)
## Section 8. Decision Tree Visualization
fig, axes = plt.subplots(1, 2, figsize=(20, 8))

plot_tree(dt, feature_names=feature_names_r, ax=axes[0],
          filled=True, rounded=True, fontsize=7, max_depth=3,
          class_names=None, impurity=False, precision=2)
axes[0].set_title('Decision Tree — Reactions\n(depth=4, msl=30, top 3 levels shown)',
                   fontweight='bold', fontsize=10)

plot_tree(dt_c, feature_names=feature_names_c, ax=axes[1],
          filled=True, rounded=True, fontsize=7, max_depth=3,
          class_names=None, impurity=False, precision=2)
axes[1].set_title('Decision Tree — Comments\n(depth=4, msl=30, top 3 levels shown)',
                  fontweight='bold', fontsize=10)

plt.tight_layout()
plt.show()

print('Decision Tree Rules (Reactions, max depth 4):')
print(export_text(dt, feature_names=feature_names_r, max_depth=4))
## Section 9. Results Summary Table
print('='*80)
print('FINAL RESULTS SUMMARY')
print('='*80)

def summary_df(results):
    rows = []
    for r in results:
        rows.append({
            'Model': r['label'],
            'Log R²': round(r['log_r2'], 4),
            'Log MAE': round(r['log_mae'], 4),
            'MAE (orig)': round(r['mae'], 1),
            'RMSE (orig)': round(r['rmse'], 1),
            'R² (orig)': round(r['r2'], 4),
            'sMAPE (%)': round(r['smape'], 1),
            'MedAE (orig)': round(r['medae'], 1),
            'Spearman ρ': round(r['spearman_rho'], 4),
        })
    return pd.DataFrame(rows).set_index('Model')

print('\nREACTIONS:')
df_r = summary_df(results_reactions)
print(df_r.to_string())

print('\nCOMMENTS:')
df_c = summary_df(results_comments)
print(df_c.to_string())

best_r = df_r['Log R²'].idxmax()
best_c = df_c['Log R²'].idxmax()
print(f'\nBest for reactions: {best_r}  (Log R²={df_r.loc[best_r, "Log R²"]}  Spearman ρ={df_r.loc[best_r, "Spearman ρ"]})')
print(f'Best for comments:  {best_c}  (Log R²={df_c.loc[best_c, "Log R²"]}  Spearman ρ={df_c.loc[best_c, "Spearman ρ"]})')
## Section 10. Predict on New Sample
def predict_engagement(
    followers=10_000,
    connections=500,
    time_spent=30,
    num_hashtags=3,
    media_type='post',
    content="Excited to share that I've just been promoted to Senior Engineer! This journey has been incredible.",
    headline='Senior Software Engineer',
    num_content_links=0,
):
    """
    Predict reactions and comments for a new LinkedIn post.
    Author baseline always uses the global LOO mean — author identity
    is not available at inference time.
    Each model uses its own LOO stats (reactions-based vs comments-based).
    Sample index is set to -1 so it never collides with training-set indices
    in the per-row LOO lookup, triggering the global-mean fallback correctly.
    """
    sample = pd.DataFrame(
        [{
            'followers':     followers,
            'connections':   connections,
            'time_spent':    time_spent,
            'num_hashtags':  num_hashtags,
            'media_type':    media_type,
            'content':       content,
            'headline':      headline,
            'content_links': np.nan if num_content_links == 0 else '; '.join(['http://example.com'] * num_content_links),
            'hashtags':      np.nan,
            'name':          '__unknown__',
        }],
        index=[-1]   # sentinel — not in any training-set LOO per-row dict
    )

    # Reactions model
    X_new_r = engineer_features(sample, loo_stats_r).fillna(0)
    for col in feature_names_r:
        if col not in X_new_r.columns:
            X_new_r[col] = 0
    X_new_r = X_new_r[feature_names_r]

    # Comments model
    X_new_c = engineer_features(sample, loo_stats_c).fillna(0)
    for col in feature_names_c:
        if col not in X_new_c.columns:
            X_new_c[col] = 0
    X_new_c = X_new_c[feature_names_c]

    pred_r_log = best_model_r.predict(X_new_r)[0]
    pred_c_log = rf_plus_c.predict(X_new_c)[0]
    pred_reactions = max(0, np.expm1(pred_r_log))
    pred_comments  = max(0, np.expm1(pred_c_log))

    print(f"{followers:,} followers  |  {media_type}  |  {num_hashtags} hashtags")
    print(f"  Predicted reactions: {pred_reactions:.0f}  (log={pred_r_log:.3f})")
    print(f"  Predicted comments:  {pred_comments:.0f}   (log={pred_c_log:.3f})")
    return pred_reactions, pred_comments

print(f"predict_engagement uses: reactions={best_r_name}  comments=RF+\n")

# ── Example 1: Large account, personal vulnerability story ────────────────────
print("--- Example 1: Large account, personal story ---")
predict_engagement(
    followers=150_000, num_hashtags=5, media_type='post',
    content='After 10 years I finally failed. And that failure taught me everything. I was scared to share this but here it is...'
)

# ── Example 2: Mid-size account, article ──────────────────────────────────────
print("\n--- Example 2: Mid-size account, article ---")
predict_engagement(
    followers=25_000, num_hashtags=4, media_type='article',
    content='How AI is transforming enterprise software. What every CTO needs to know about machine learning adoption in 2025.'
)

# ── Example 3: Small account, announcement ────────────────────────────────────
print("\n--- Example 3: Small account, announcement post ---")
predict_engagement(
    followers=8_000, num_hashtags=2, media_type='post',
    content="Excited to share that I've just been promoted to Senior Engineer! This journey has been incredible."
)